{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.analysis_helpers import *\n",
    "from utils.clusters_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the data to analyse\n",
    "- Drop or select specifics conditions\n",
    "- Differentiate between interviwer & participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../interviews_corrected/6_final/**/' \n",
    "\n",
    "df_all = load_and_combine_csv(directory)\n",
    "df_all = standardize_data(df_all)\n",
    "df_all = calculate_word_counts(df_all)\n",
    "\n",
    "df_all.to_csv(\"outputs/combined_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique conditions before filtering: {df_all['Condition'].unique()}\")\n",
    "print(f\"Number of interviews before filtering: {df_all['File Name'].nunique()}\")\n",
    "# *0*: No \"real\" interview (e.g., setup phase, small talk). We filter these out.\n",
    "df_all = df_all[df_all[\"Condition\"] != 0]\n",
    "print(f\"Unique conditions after filtering: {df_all['Condition'].unique()}\")\n",
    "print(f\"Number of interviews (File Name) after filtering: {df_all['File Name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we only want to focus on the participant speaker\n",
    "df_participant = df_all[df_all[\"Speaker\"] == \"Participant\"].copy()\n",
    "df_interviewer = df_all[df_all[\"Speaker\"] == \"Interviewer\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new row for the \"All\" category\n",
    "df_word = aggregate_counts(df_participant,[\"Experiment\",\"Id\"]).copy()\n",
    "df_word['Experiment'] = 'All'\n",
    "\n",
    "# Concatenate the original data with the \"All\" data\n",
    "df_combined = pd.concat([df_word,aggregate_counts(df_all,[\"Experiment\",\"Id\"])])\n",
    "\n",
    "stripplot(df_combined, 'Experiment', 'Word Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Id instead of File Name because the same participant can have the same condition in different interviews\n",
    "stripplot_with_counts(aggregate_counts(df_participant, ['Experiment', \"Id\", \"Condition\"]), 'Experiment', 'Word Count', \n",
    "                      hue_column='Condition', id_column=\"Id\", legend_labels=[\"Only one interview conducted for the participant\",\"Control\",\"Intervention\"] ,file_name=\"outputs/stripplot_word_count_id.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripplot_with_counts(aggregate_counts(df_all, ['Experiment', \"Id\", 'Speaker']), 'Experiment', 'Word Count', hue_column='Speaker', id_column=\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interviewers Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop participants and focus on interviewers\n",
    "filtered_df = df_all[df_all[\"Speaker_original\"].str.contains(\"Interviewer\")]\n",
    "\n",
    "# Step 2: Count unique interviewers per interview\n",
    "experimenter_count = (\n",
    "    filtered_df.groupby(['Experiment', 'File Name'])['Speaker_original']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    ")\n",
    "experimenter_count.rename(columns={'Speaker_original': 'Number of Interviewers'}, inplace=True)\n",
    "\n",
    "# Step 3: Aggregate counts for each experiment and interviewer category\n",
    "experiment_summary = experimenter_count.groupby(['Experiment', 'Number of Interviewers'])['File Name'].count().reset_index()\n",
    "experiment_summary.rename(columns={'File Name': 'Count'}, inplace=True)\n",
    "\n",
    "# Step 4: Ensure all categories are represented\n",
    "categories = list(range(1, experimenter_count['Number of Interviewers'].max() + 1))  # 1, 2, 3, etc.\n",
    "all_experiments = experiment_summary['Experiment'].unique()\n",
    "full_summary = pd.DataFrame(\n",
    "    [(experiment, category) for experiment in all_experiments for category in categories],\n",
    "    columns=['Experiment', 'Number of Interviewers']\n",
    ").merge(experiment_summary, on=['Experiment', 'Number of Interviewers'], how='left').fillna(0)\n",
    "\n",
    "# Convert counts to integers for clarity\n",
    "full_summary['Count'] = full_summary['Count'].astype(int)\n",
    "\n",
    "# Convert the Number of Interviewers to string for grouped bar chart\n",
    "full_summary['Number of Interviewers'] = full_summary['Number of Interviewers'].astype(str)\n",
    "\n",
    "# Step 5: Stacked bar chart\n",
    "stacked_data = full_summary.pivot(index='Experiment', columns='Number of Interviewers', values='Count').fillna(0)\n",
    "stacked_data.plot(kind='bar', stacked=True, figsize=(10, 6), width=0.8, linewidth=0)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Distribution of Number of Interviewers per Experiment\")\n",
    "plt.ylabel(\"Number of Interviews\")\n",
    "plt.xlabel(\"Experiment\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"Number of Interviewer \\n per Experiment\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Drop participants and focus on interviewers\n",
    "filtered_df = df_all[df_all[\"Speaker_original\"].str.contains(\"Interviewer\")]\n",
    "\n",
    "# Step 2: Count unique interviewers per interview\n",
    "experimenter_count = (\n",
    "    filtered_df.groupby(['Experiment', 'File Name'])['Speaker_original']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    ")\n",
    "experimenter_count.rename(columns={'Speaker_original': 'Number of Interviewers'}, inplace=True)\n",
    "\n",
    "# Step 3: Filter for interviews with more than 1 interviewer\n",
    "multi_interviewer_files = experimenter_count[experimenter_count['Number of Interviewers'] > 1]\n",
    "filtered_df = filtered_df[filtered_df['File Name'].isin(multi_interviewer_files['File Name'])]\n",
    "\n",
    "# Step 4: Calculate total word count per speaker per interview\n",
    "word_distribution = filtered_df.groupby(['File Name', 'Speaker_original', 'Experiment'])['Word Count'].sum().reset_index()\n",
    "\n",
    "# Step 5: Calculate dominance (largest contribution)\n",
    "dominance = word_distribution.groupby(['File Name', 'Experiment'])['Word Count'].max().reset_index()\n",
    "total_word_count = word_distribution.groupby(['File Name'])['Word Count'].sum().reset_index()\n",
    "dominance = dominance.merge(total_word_count, on='File Name', how='left')\n",
    "dominance['Dominance (%)'] = (dominance['Word Count_x'] / dominance['Word Count_y']) * 100\n",
    "dominance.rename(columns={'Word Count_x': 'Max Contribution', 'Word Count_y': 'Total Words'}, inplace=True)\n",
    "\n",
    "# Sort by dominance\n",
    "dominance = dominance.sort_values(by='Dominance (%)', ascending=False)\n",
    "\n",
    "# Step 6: Plot dominance for interviews with more than 1 interviewer\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=dominance, x='Dominance (%)', y='File Name', hue='Experiment', palette=\"Set2\")\n",
    "\n",
    "# Add a vertical line at 50% to highlight balance\n",
    "plt.axvline(50, color='black', linestyle='--', label=\"50% Threshold\")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Contribution of Most Present Interviewer for Interviews with More Than 1 Interviewer\", fontsize=16)\n",
    "plt.xlabel(\"Contribution (%)\", fontsize=12)\n",
    "plt.ylabel(\"Interview (File Name)\", fontsize=12)\n",
    "plt.legend(title=\"Experiment\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplest version for the power-point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "sns.boxplot(data=dominance, y='Dominance (%)', color='lightgray')\n",
    "sns.stripplot(data=dominance, y='Dominance (%)', hue=\"Experiment\", jitter=True, edgecolor='k', linewidth=1, palette=\"Set2\")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Contribution of the most present Interviewer \", fontsize=16)\n",
    "plt.ylabel(\"Contribution (%)\", fontsize=12)\n",
    "# put legend outside of the plot\n",
    "plt.legend(title=\"Experiment\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/contribution_interviewer.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter out participants and focus on interviewers\n",
    "filtered_df = df_all[df_all[\"Speaker_original\"].str.contains(\"Interviewer\")]\n",
    "\n",
    "# Step 2: Count the number of unique interviewers for each interview\n",
    "interviewer_counts = (\n",
    "    filtered_df.groupby(['Id', 'File Name'])['Speaker_original']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    ")\n",
    "interviewer_counts.rename(columns={'Speaker_original': 'Number of Interviewers'}, inplace=True)\n",
    "\n",
    "# Step 3: Check changes in the number of interviewers for each participant (ID)\n",
    "participant_interviewer_changes = (\n",
    "    interviewer_counts.groupby('Id')['Number of Interviewers']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    ")\n",
    "participant_interviewer_changes.rename(columns={'Number of Interviewers': 'Unique Interviewer Counts'}, inplace=True)\n",
    "\n",
    "# Step 4: Identify participants with changes in interviewer counts across their interviews\n",
    "participants_with_changes = participant_interviewer_changes[\n",
    "    participant_interviewer_changes['Unique Interviewer Counts'] > 1\n",
    "]\n",
    "\n",
    "# Step 5: Display results\n",
    "if participants_with_changes.empty:\n",
    "    print(\"All participants have consistent numbers of interviewers across their interviews.\")\n",
    "else:\n",
    "    print(\"The following participants have varying numbers of interviewers across their interviews:\")\n",
    "    print(participants_with_changes)\n",
    "\n",
    "# Optional: Display detailed changes per ID\n",
    "detailed_changes = interviewer_counts[interviewer_counts['Id'].isin(participants_with_changes['Id'])]\n",
    "print(\"\\nDetailed changes for inconsistent participants:\")\n",
    "print(detailed_changes.sort_values(by=['Id', 'File Name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop participants and focus on interviewers\n",
    "filtered_df = df_all[df_all[\"Speaker_original\"].str.contains(\"Interviewer\")]\n",
    "\n",
    "# Step 2: Count unique interviewers per interview\n",
    "experimenter_count = (\n",
    "    filtered_df.groupby(['Experiment', 'File Name'])['Speaker_original']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    ")\n",
    "experimenter_count.rename(columns={'Speaker_original': 'Number of Interviewers'}, inplace=True)\n",
    "\n",
    "# Step 3: Filter for interviews with more than 1 interviewer\n",
    "multi_interviewer_files = experimenter_count[experimenter_count['Number of Interviewers'] > 1]\n",
    "filtered_df = filtered_df[filtered_df['File Name'].isin(multi_interviewer_files['File Name'])]\n",
    "\n",
    "# Step 4: Calculate total word count per speaker per interview\n",
    "word_distribution = filtered_df.groupby(['File Name', 'Speaker_original', 'Experiment'])['Word Count'].sum().reset_index()\n",
    "\n",
    "# Step 5: Calculate dominance (largest contribution)\n",
    "dominance = word_distribution.groupby(['File Name', 'Experiment'])['Word Count'].max().reset_index()\n",
    "total_word_count = word_distribution.groupby(['File Name'])['Word Count'].sum().reset_index()\n",
    "dominance = dominance.merge(total_word_count, on='File Name', how='left')\n",
    "dominance['Dominance (%)'] = (dominance['Word Count_x'] / dominance['Word Count_y']) * 100\n",
    "dominance.rename(columns={'Word Count_x': 'Max Contribution', 'Word Count_y': 'Total Words'}, inplace=True)\n",
    "\n",
    "# Sort by dominance\n",
    "dominance = dominance.sort_values(by='Dominance (%)', ascending=False)\n",
    "\n",
    "# Step 6: Identify interviews to highlight\n",
    "inconsistent_interviews = detailed_changes['File Name'].unique()  # From earlier inconsistent participant analysis\n",
    "special_case = 'S302'  # Special case to highlight differently\n",
    "\n",
    "# Step 7: Plot dominance for interviews with more than 1 interviewer\n",
    "plt.figure(figsize=(12, 8))\n",
    "barplot = sns.barplot(data=dominance, x='Dominance (%)', y='File Name', hue='Experiment', palette=\"Set2\")\n",
    "\n",
    "# Add a vertical line at 50% to highlight balance\n",
    "plt.axvline(50, color='black', linestyle='--', label=\"50% Threshold\")\n",
    "\n",
    "# Customize y-axis tick labels\n",
    "for tick, label in zip(barplot.get_yticks(), barplot.get_yticklabels()):\n",
    "    file_name = label.get_text()\n",
    "    if file_name in inconsistent_interviews and special_case not in file_name :\n",
    "        label.set_color('red')  # Highlight inconsistent interviews in red\n",
    "        label.set_fontweight('bold')\n",
    "    elif special_case in file_name:\n",
    "        label.set_color('blue')  # Highlight the special case in blue\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Dominance of Most Present Interviewer for Interviews with More Than 1 Interviewer\", fontsize=16)\n",
    "plt.xlabel(\"Dominant Contribution (%)\", fontsize=12)\n",
    "plt.ylabel(\"Interview (File Name)\", fontsize=12)\n",
    "plt.legend(title=\"Experiment\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to classify the points for hue\n",
    "dominance['Highlight'] = dominance['File Name'].apply(\n",
    "    lambda x: 'Variable' if special_case in x or x in inconsistent_interviews else \n",
    "              'Consistent'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Step 1: Create a base boxplot\n",
    "sns.boxplot(data=dominance, y='Dominance (%)', color='lightgray')\n",
    "\n",
    "# Step 2: Add a stripplot with hue for highlighting\n",
    "sns.stripplot(\n",
    "    data=dominance,\n",
    "    y='Dominance (%)',\n",
    "    hue='Highlight',  # Use the new column to distinguish points\n",
    "    jitter=True,\n",
    "    edgecolor='k',\n",
    "    linewidth=1,\n",
    "    alpha=0.8,\n",
    "    palette={'Variable': 'red', 'Consistent': 'gray'},\n",
    ")\n",
    "\n",
    "# Step 3: Titles and labels\n",
    "plt.title(\"Contribution of the most present Interviewer\", fontsize=16)\n",
    "plt.ylabel(\"Contribution (%)\", fontsize=12)\n",
    "\n",
    "# Step 4: Adjust legend\n",
    "plt.legend(title=\"Numbers of interviewers\\nfor the same participant\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "\n",
    "# Step 5: Tight layout and save\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/contribution_interviewer_highlighted.png\", dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter out participants and focus on interviewers\n",
    "filtered_df = df_all[df_all[\"Speaker_original\"].str.contains(\"Interviewer\")]\n",
    "\n",
    "# Step 2: Count the number of unique interviewers for each interview\n",
    "interviewer_counts = (\n",
    "    filtered_df.groupby(['Id', 'File Name'])['Speaker_original']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    ")\n",
    "interviewer_counts.rename(columns={'Speaker_original': 'Number of Interviewers'}, inplace=True)\n",
    "\n",
    "# Step 3: Check changes in the number of interviewers for each participant (ID)\n",
    "participant_interviewer_changes = (\n",
    "    interviewer_counts.groupby('Id')['Number of Interviewers']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    ")\n",
    "participant_interviewer_changes.rename(columns={'Number of Interviewers': 'Unique Interviewer Counts'}, inplace=True)\n",
    "\n",
    "# Step 4: Identify participants with changes in interviewer counts across their interviews\n",
    "participants_with_changes = participant_interviewer_changes[\n",
    "    participant_interviewer_changes['Unique Interviewer Counts'] > 1\n",
    "]\n",
    "\n",
    "# Step 5: Filter detailed changes for inconsistent participants\n",
    "detailed_changes = interviewer_counts[interviewer_counts['Id'].isin(participants_with_changes['Id'])]\n",
    "\n",
    "# Step 6: Filter the main DataFrame for inconsistent participants\n",
    "inconsistent_interviews = filtered_df[filtered_df['Id'].isin(participants_with_changes['Id'])]\n",
    "\n",
    "# Step 7: Calculate total word count per speaker per interview\n",
    "word_distribution = inconsistent_interviews.groupby(['File Name', 'Speaker_original'])['Word Count'].sum().reset_index()\n",
    "\n",
    "# Step 8: Calculate total word count per interview\n",
    "total_words_per_interview = word_distribution.groupby('File Name')['Word Count'].sum().reset_index()\n",
    "total_words_per_interview.rename(columns={'Word Count': 'Total Words'}, inplace=True)\n",
    "\n",
    "# Step 9: Merge total word counts to calculate percentages\n",
    "word_distribution = word_distribution.merge(total_words_per_interview, on='File Name')\n",
    "word_distribution['Percentage'] = (word_distribution['Word Count'] / word_distribution['Total Words']) * 100\n",
    "\n",
    "# Step 10: Pivot to create a DataFrame with rows as interviews and columns as speakers\n",
    "word_distribution_pivot = word_distribution.pivot(index='File Name', columns='Speaker_original', values='Percentage').fillna(0)\n",
    "\n",
    "# Step 11: Plot stacked bar chart for inconsistent interviews\n",
    "plt.figure(figsize=(12, 6))\n",
    "word_distribution_pivot.plot(kind=\"bar\", stacked=True, figsize=(12, 6), width=0.8, linewidth=0)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Percentage Contribution of Interviewers for Interviews with Inconsistent Participants\")\n",
    "plt.xlabel(\"Interview (File Name)\")\n",
    "plt.ylabel(\"Percentage Contribution (%)\")\n",
    "plt.xticks(rotation=90, ha=\"center\")\n",
    "plt.legend(title=\"Speaker\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter out participants and focus on interviewers\n",
    "filtered_df = df_all[df_all[\"Speaker_original\"].str.contains(\"Interviewer\")]\n",
    "\n",
    "# Step 2: Count unique interviewers per interview and filter for interviews with more than 1 interviewer\n",
    "experimenter_count = (\n",
    "    filtered_df.groupby(['File Name'])['Speaker_original']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    ")\n",
    "experimenter_count.rename(columns={'Speaker_original': 'Number of Interviewers'}, inplace=True)\n",
    "\n",
    "# Step 3: Filter for interviews with more than 1 interviewer\n",
    "multi_interviewer_files = experimenter_count[experimenter_count['Number of Interviewers'] > 1]['File Name']\n",
    "filtered_df = filtered_df[filtered_df['File Name'].isin(multi_interviewer_files)]\n",
    "\n",
    "# Step 4: Calculate total word count per speaker per interview\n",
    "word_distribution = filtered_df.groupby(['File Name', 'Speaker_original'])['Word Count'].sum().reset_index()\n",
    "\n",
    "# Step 5: Calculate total word count per interview\n",
    "total_words_per_interview = word_distribution.groupby('File Name')['Word Count'].sum().reset_index()\n",
    "total_words_per_interview.rename(columns={'Word Count': 'Total Words'}, inplace=True)\n",
    "\n",
    "# Step 6: Merge total word counts to calculate percentages\n",
    "word_distribution = word_distribution.merge(total_words_per_interview, on='File Name')\n",
    "word_distribution['Percentage'] = (word_distribution['Word Count'] / word_distribution['Total Words']) * 100\n",
    "\n",
    "# Step 7: Pivot to create a DataFrame with rows as interviews and columns as speakers\n",
    "word_distribution_pivot = word_distribution.pivot(index='File Name', columns='Speaker_original', values='Percentage').fillna(0)\n",
    "\n",
    "# Step 8: Plot stacked bar chart for interviews with more than 1 interviewer\n",
    "plt.figure(figsize=(12, 6))\n",
    "word_distribution_pivot.plot(kind=\"bar\", stacked=True, figsize=(12, 6), width=0.8,linewidth=0)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Percentage Contribution of Interviewers for Interviews with More Than 1 Interviewer\")\n",
    "plt.xlabel(\"Interview (File Name)\")\n",
    "plt.ylabel(\"Percentage Contribution (%)\")\n",
    "plt.xticks(rotation=90, ha=\"center\")\n",
    "plt.legend(title=\"Speaker\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency & Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stopwords = [\"yeah\",\"like\",\"think\",\"know\",\"dont\",\"yes\",\"okay\",\"mm\"\"really\",\"bit\",\"could\",\"that's\",\n",
    "                   \"see\",\"feel\",\"felt\",\"ah\",\"oh\",\"also\",\"I've\",\"maybe\",\"was\",\"thinking\",\"thought\",\"really\",\"thing\",\"part\"]\n",
    "\n",
    "df_participant['preprocessed_content'] = df_participant['Content'].apply(lambda x: preprocess_text(x, extra_stopwords=extra_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_analysis(df_participant, top_n=30,groupby_column=[\"Experiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared participants uniques words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can specify any list of columns for grouping, e.g., ['Experiment', 'Condition']\n",
    "unique_words_df_participant = count_unique_words(df_participant, groupby_columns=['Experiment'])#, 'Condition'])\n",
    "unique_words_df_participant = unique_words_df_participant.sort_values(by='Participant_Count', ascending=False)\n",
    "unique_words_df_participant.to_csv(\"outputs/unique_words_participant.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_clouds(unique_words_df_participant, max_words=25, groupby_columns=['Experiment'])#,\"Condition\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Interviewers uniques words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interviewer['preprocessed_content'] = df_interviewer['Content'].apply(lambda x: preprocess_text(x, extra_stopwords=extra_stopwords))\n",
    "\n",
    "unique_words_df_interviewer = count_unique_words(df_interviewer, groupby_columns=['Experiment'])#, 'Condition'])\n",
    "unique_words_df_interviewer = unique_words_df_interviewer.sort_values(by='Participant_Count', ascending=False)\n",
    "unique_words_df_interviewer.to_csv(\"outputs/unique_words_interviewer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_clouds(unique_words_df_interviewer, groupby_columns=['Experiment'], max_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
