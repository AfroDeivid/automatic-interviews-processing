{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of accuracy of the transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to assess the performance of predictions after manual verification. *(Also providing enhanced visual HTML files to help interpret metrics clearly.)*\n",
    "\n",
    "- After manually verifying transcripts, you can use the verified transcripts as reference (ground truth) data to test the accuracy of various models.\n",
    "    \n",
    "- **However**, it's essential to note that if the reference file (ground truth) was created by using a modelâ€™s predictions as a template the ground truth may carry a bias towards that initial model. *For instance, if a model omitted interruptions or adjusted sentence structure and the verifier found these changes accurate in context, these modifications might remain, unintentionally favoring that model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from utils.evaluation_helpers import process_folder_csv, load_data_time, compute_der, process_folder_text\n",
    "from utils.format_helpers import get_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions vs References: Performance evaluation (ASR & Diarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing evaluation folder (Predictions vs References)\n",
    "\n",
    "Useful if want to test/compare differents models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['../results/Compassion', '../results/OBE1', '../results/OBE2'] # The csv file that you want to compare\n",
    "pred_folder = '../evaluation/predictions' # Specify the name if trying to compare different ASR/Diarization models.\n",
    "\n",
    "def copy_csv_files(directories, pred_folder):\n",
    "    pred_files = []\n",
    "    for directory in directories:\n",
    "        pred_files.append(get_files(directory, 'csv'))\n",
    "    pred_files = [item for sublist in pred_files for item in sublist]\n",
    "\n",
    "    os.makedirs(pred_folder, exist_ok=True)\n",
    "    for file in pred_files:\n",
    "        shutil.copy(file, pred_folder)\n",
    "\n",
    "copy_csv_files(directories, pred_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Files\n",
    "\n",
    "When corrections are made directly in the CSV file and timestamps are adjusted, it ensures a more robust computation of the DER (Diarization Error Rate) metric. However, for simplicity and efficiency during the verification process, the **Text File** method described below is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR and Diarization metrics + Visual Tool to easily understand missmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_folder = \"../evaluation/predictions\"\n",
    "reference_folder = \"../evaluation/references\"\n",
    "\n",
    "# Provide the original duration of the audio files.\n",
    "# This ensures the diarization error rate (DER) metric accounts for the total audio duration, not just the speech segments detected by the model.\n",
    "# If this information is missing, DER calculations may be inaccurate as they consider only the detected speech intervals, worsening the DER metric.\n",
    "orignal_info = pd.read_csv(\"./outputs/audio_data.csv\")\n",
    "\n",
    "metric = process_folder_csv(prediction_folder, reference_folder, info=orignal_info)\n",
    "metric.to_csv('../evaluation/metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Dialogue DER Analysis in CSV instead than in HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogue_DER(reference_file, prediction_file, output_file=None):\n",
    "    df_ref, df_pred = load_data_time(reference_file, prediction_file)\n",
    "    dialogue_df, error_durations = compute_der(df_ref, df_pred)\n",
    "\n",
    "    # Display error durations and DER\n",
    "    print(\"\\nError Durations and DER:\")\n",
    "    for key, value in error_durations.items():\n",
    "        if key == 'DER':\n",
    "            print(f\"{key}: {value:.2%}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value:.3f} seconds\")\n",
    "\n",
    "    if output_file:\n",
    "        dialogue_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_file = \"../evaluation/references/S301final.csv\"\n",
    "prediction_file = \"../evaluation/predictions/S301final.csv\"\n",
    "output_file = '../outputs/dia_S301final.csv'\n",
    "\n",
    "dialogue_DER(reference_file, prediction_file)#, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Files \n",
    "When corrections are made directly in a text file rather than adjusting timestamps in a CSV (as was done for the Grief dataset interviews), a simplified approach to DER is required:\n",
    "\n",
    "Since time segments are not adjusted, we ignore missed speech and false alarm errors. Instead, we focus solely on confusion errors, defined as the number of words reassigned from one speaker to another. *This replaces the traditional time-based DER calculations with a straightforward, text-based method.*\n",
    "\n",
    "**Note:** This approach can be preferred as it accelerates the manual correction process by providing a better visual representation, with segments concatenated by turn in the text file and by neglecting the need to correct timestamps, it offers significant time savings without compromising the core analysis.\n",
    "\n",
    "**Assumptions:**\n",
    "\n",
    "- During manual verification, brackets (``[]``) are exclusively used to denote speaker labels and are not added for any other purpose.\n",
    "- Speaker names/labels within the brackets remain consistent throughout the corrections.\n",
    "\n",
    "These assumptions ensure the algorithm can process the text reliably while maintaining the integrity of speaker assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_folder = \"../evaluation/predictions_text\"\n",
    "reference_folder = \"../evaluation/references_text\"\n",
    "\n",
    "metric = process_folder_text(prediction_folder, reference_folder,dir_visual=\"meditation_visual_text\")\n",
    "metric.to_csv('../evaluation/meditation_metrics_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meditations Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of Filename to Experiment\n",
    "df_audio = pd.read_csv('./outputs/audio_data.csv')\n",
    "filename_to_experiment = dict(zip(df_audio['File Name'],df_audio['Experiment']))\n",
    "\n",
    "df = pd.read_csv('../evaluation/metrics.csv')\n",
    "df['Experiment'] = df['Filename'].map(filename_to_experiment)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(df, title=None, file_name=None):\n",
    "    \"\"\"\n",
    "    Plots box plots for WER and DER.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the metrics.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    # Melt the DataFrame for easier plotting with Seaborn\n",
    "    melted_df = df.melt(id_vars='Filename', value_vars=['WER', 'DER'], var_name='Metric', value_name='Value')\n",
    "    ax = sns.boxplot(x='Metric', y='Value',hue=\"Metric\" ,data=melted_df,)# palette=['skyblue', 'salmon'])\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.xlabel('')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    \n",
    "    # Calculate and print medians\n",
    "    medians = melted_df.groupby('Metric')['Value'].median()\n",
    "    print(\"Median Values:\")\n",
    "    for metric, median_value in medians.items():\n",
    "        print(f\"  {metric}: {median_value:.3f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if file_name:\n",
    "        plt.savefig(file_name, dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(df, file_name='images/meditation_metrics_time.png',title='Meditation with time-stamps DER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot_experiment(df):\n",
    "    \"\"\"\n",
    "    Plots box plots for WER and DER, grouped by Experiment.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the metrics and Experiment column.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Melt the DataFrame for easier plotting with Seaborn\n",
    "    melted_df = df.melt(id_vars=['Filename', 'Experiment'], \n",
    "                        value_vars=['WER', 'DER'], \n",
    "                        var_name='Metric', \n",
    "                        value_name='Value')\n",
    "    \n",
    "    # Create the boxplot with Experiment as hue\n",
    "    sns.boxplot(x='Metric', y='Value', hue='Experiment', data=melted_df, palette='pastel')\n",
    "    sns.stripplot(x='Metric', y='Value', hue='Experiment', data=melted_df, size=4, linewidth=1,dodge=True, jitter=True, legend=False, edgecolor=\"k\")\n",
    "    \n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.xlabel('')\n",
    "    #plt.title('Boxplots of WER and DER by Experiment')\n",
    "    plt.legend(title='Experiment')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot_experiment(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grief & Meditation Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g = pd.read_csv('../evaluation/grief_metrics_text.csv')\n",
    "df_m = pd.read_csv('../evaluation/med_metrics_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g[\"Group\"] = \"Grief\"\n",
    "df_m[\"Group\"] = \"Meditation\"\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "df_text = pd.concat([df_m, df_g], ignore_index=True)\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(df_g, title=\"Grief Text\", file_name=\"images/grief_text_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(df_m,title=\"Meditation Text\", file_name=\"images/meditation_text_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots_group(df, tittle=None, file_name=None):\n",
    "    \"\"\"\n",
    "    Plots two boxplots in subplots for WER and DER, grouped by the 'Group' column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the metrics.\n",
    "    \"\"\"\n",
    "    # Melt the DataFrame for easier plotting with Seaborn\n",
    "    melted_df = df.melt(\n",
    "        id_vars=['Filename', 'Group'], \n",
    "        value_vars=['WER', 'DER'], \n",
    "        var_name='Metric', \n",
    "        value_name='Value',\n",
    "    )\n",
    "    \n",
    "    # Calculate medians grouped by Metric and Group\n",
    "    medians = melted_df.groupby(['Metric', 'Group'])['Value'].median()\n",
    "    print(\"Median Values:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(medians.to_string())\n",
    "\n",
    "    \n",
    "    # Create the subplots\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(8, 10), sharey=False)\n",
    "    \n",
    "    # Plot WER boxplot\n",
    "    sns.boxplot(\n",
    "        ax=axes[0],\n",
    "        x='Group', y='Value', \n",
    "        data=melted_df[melted_df['Metric'] == 'WER'], \n",
    "        hue='Group'\n",
    "    )\n",
    "    axes[0].set_title('WER by Dataset')\n",
    "    axes[0].set_ylabel('WER')\n",
    "    axes[0].set_xlabel('')\n",
    "    \n",
    "    # Plot DER boxplot\n",
    "    sns.boxplot(\n",
    "        ax=axes[1],\n",
    "        x='Group', y='Value', \n",
    "        data=melted_df[melted_df['Metric'] == 'DER'], \n",
    "        hue='Group'\n",
    "    )\n",
    "    axes[1].set_title('DER by Dataset')\n",
    "    axes[1].set_ylabel('DER')\n",
    "    axes[1].set_xlabel('')\n",
    "\n",
    "    if tittle:\n",
    "        fig.suptitle(tittle, fontsize=16)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    if file_name:\n",
    "        plt.savefig(file_name, dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots_group(df_text, file_name=\"images/comparaison_Grief_Meditation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vamos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
