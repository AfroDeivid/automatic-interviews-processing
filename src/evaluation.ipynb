{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the transcripts: Metrics & time passed to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first part of this notebook is designed to assess the performance of predictions after manual verification. *(Also providing enhanced visual HTML files to help interpret metrics clearly.)*\n",
    "\n",
    "    - After manually verifying transcripts, you can use the verified transcripts as reference (ground truth) data to test the accuracy of various models.\n",
    "    \n",
    "    - **However**, it's essential to note that if the reference file (ground truth) was created by using a modelâ€™s predictions as a template the ground truth may carry a bias towards that initial model. *For instance, if a model omitted interruptions or adjusted sentence structure and the verifier found these changes accurate in context, these modifications might remain, unintentionally favoring that model.*\n",
    "\n",
    "- The second part of the notebook focuses on tracking the time spent verifying each transcript. This data allows you to quantify time savings, providing insights into the efficiency of each model based on verification time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from utils.evaluation_helpers import process_folder, load_data_time, compute_der\n",
    "from utils.format_helpers import get_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions vs References: Performance evaluation (ASR & Diarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing evaluation folder (Predictions vs References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usefull if want to test/compare differents models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['../results/Compassion', '../results/OBE1', '../results/OBE2'] # The csv file that you want to compare\n",
    "pred_folder = '../evaluation/predictions' # Specify the name if trying to compare different ASR/Diarization models.\n",
    "\n",
    "def copy_csv_files(directories, pred_folder):\n",
    "    pred_files = []\n",
    "    for directory in directories:\n",
    "        pred_files.append(get_files(directory, 'csv'))\n",
    "    pred_files = [item for sublist in pred_files for item in sublist]\n",
    "\n",
    "    os.makedirs(pred_folder, exist_ok=True)\n",
    "    for file in pred_files:\n",
    "        shutil.copy(file, pred_folder)\n",
    "\n",
    "copy_csv_files(directories, pred_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR and Diarization metrics + Visual Tool to easily understand missmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_folder = \"../evaluation/predictions\"\n",
    "reference_folder = \"../evaluation/references\"\n",
    "\n",
    "# Provide the original duration of the audio files.\n",
    "# This ensures the diarization error rate (DER) metric accounts for the total audio duration, not just the speech segments detected by the model.\n",
    "# If this information is missing, DER calculations may be inaccurate as they consider only the detected speech intervals, worsening the DER metric.\n",
    "orignal_info = pd.read_csv(\"./outputs/audio_data.csv\")\n",
    "\n",
    "metric = process_folder(prediction_folder, reference_folder, info=orignal_info)\n",
    "metric.to_csv('../evaluation/metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Dialogue DER Analysis in CSV instead than in HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogue_DER(reference_file, prediction_file, output_file):\n",
    "    df_ref, df_pred = load_data_time(reference_file, prediction_file)\n",
    "    dialogue_df, error_durations = compute_der(df_ref, df_pred)\n",
    "\n",
    "    # Display error durations and DER\n",
    "    print(\"\\nError Durations and DER:\")\n",
    "    for key, value in error_durations.items():\n",
    "        if key == 'DER':\n",
    "            print(f\"{key}: {value:.2%}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value:.3f} seconds\")\n",
    "\n",
    "    # Optionally, save the DataFrame to a CSV file\n",
    "    dialogue_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_file = \"../evaluation/references/S301final.csv\"\n",
    "prediction_file = \"../evaluation/predictions/S301final.csv\"\n",
    "output_file = 'dia_S301final_new.csv'\n",
    "\n",
    "dialogue_DER(reference_file, prediction_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of Filename to Experiment\n",
    "df_audio = pd.read_csv('./outputs/audio_data.csv')\n",
    "filename_to_experiment = dict(zip(df_audio['File Name'],df_audio['Experiment']))\n",
    "\n",
    "df = pd.read_csv('../evaluation/metrics.csv')\n",
    "df['Experiment'] = df['Filename'].map(filename_to_experiment)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(df):\n",
    "    \"\"\"\n",
    "    Plots histograms for WER and DER distributions.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the metrics.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Define bins dynamically based on the maximum values in the data\n",
    "    wer_max = df['WER'].max()\n",
    "    der_max = df['DER'].max()\n",
    "    \n",
    "    wer_bins = np.arange(0, wer_max + 0.05, 0.05)  # Bins for WER\n",
    "    der_bins = np.arange(0, der_max + 0.05, 0.05)  # Bins for DER\n",
    "    \n",
    "    # Histogram for WER\n",
    "    plt.subplot(1, 2, 1)\n",
    "    ax1 = sns.histplot(df['WER'], bins=wer_bins, kde=True, color='skyblue')\n",
    "    plt.xlabel('Word Error Rate (WER)')\n",
    "    #plt.title('Distribution of WER')\n",
    "    plt.xticks(np.arange(0, wer_max + 0.1, 0.1))  # X-ticks at 10% intervals\n",
    "\n",
    "    # Annotate each bar with percentage\n",
    "    for p in ax1.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            percentage = (height / df['WER'].shape[0]) * 100\n",
    "            ax1.annotate(f'{percentage:.1f}%', \n",
    "                         (p.get_x() + p.get_width() / 2., height),\n",
    "                         ha='center', va='bottom', fontsize=10, color='black')\n",
    "    \n",
    "    # Histogram for DER\n",
    "    plt.subplot(1, 2, 2)\n",
    "    ax2 = sns.histplot(df['DER'], bins=der_bins, kde=True, color='salmon')\n",
    "    plt.xlabel('Diarization Error Rate (DER)')\n",
    "    #plt.title('Distribution of DER')\n",
    "    plt.xticks(np.arange(0, der_max + 0.1, 0.1))  # X-ticks at 10% intervals\n",
    "\n",
    "    # Annotate each bar with percentage\n",
    "    for p in ax2.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            percentage = (height / df['DER'].shape[0]) * 100\n",
    "            ax2.annotate(f'{percentage:.1f}%', \n",
    "                         (p.get_x() + p.get_width() / 2., height),\n",
    "                         ha='center', va='bottom', fontsize=10, color='black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_boxplots(df):\n",
    "    \"\"\"\n",
    "    Plots box plots for WER and DER.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the metrics.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    # Melt the DataFrame for easier plotting with Seaborn\n",
    "    melted_df = df.melt(id_vars='Filename', value_vars=['WER', 'DER'], var_name='Metric', value_name='Value')\n",
    "    sns.boxplot(x='Metric', y='Value',hue=\"Metric\" ,data=melted_df, palette=['skyblue', 'salmon'])\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.xlabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_wer_vs_der_scatter(df):\n",
    "    \"\"\"\n",
    "    Plots an interactive scatter plot of WER vs. DER using Plotly.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the metrics, must include 'WER', 'DER', and 'Filename' columns.\n",
    "    \"\"\"\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x='WER',\n",
    "        y='DER',\n",
    "        color='Filename',\n",
    "        hover_data=['Filename','Experiment'],  # Show filenames on hover\n",
    "        labels={\n",
    "            'WER': 'Word Error Rate (WER)',\n",
    "            'DER': 'Diarization Error Rate (DER)'\n",
    "        },\n",
    "        title='Interactive Scatter Plot of WER vs. DER'\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=10))\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Word Error Rate (WER)',\n",
    "        yaxis_title='Diarization Error Rate (DER)',\n",
    "        showlegend=False\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(df)\n",
    "plot_histograms(df)\n",
    "plot_wer_vs_der_scatter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot_experiment(df):\n",
    "    \"\"\"\n",
    "    Plots box plots for WER and DER, grouped by Experiment.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the metrics and Experiment column.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Melt the DataFrame for easier plotting with Seaborn\n",
    "    melted_df = df.melt(id_vars=['Filename', 'Experiment'], \n",
    "                        value_vars=['WER', 'DER'], \n",
    "                        var_name='Metric', \n",
    "                        value_name='Value')\n",
    "    \n",
    "    # Create the boxplot with Experiment as hue\n",
    "    sns.boxplot(x='Metric', y='Value', hue='Experiment', data=melted_df, palette='pastel')\n",
    "    sns.stripplot(x='Metric', y='Value', hue='Experiment', data=melted_df, size=4, linewidth=1,dodge=True, jitter=True, legend=False, edgecolor=\"k\")\n",
    "    \n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.xlabel('')\n",
    "    #plt.title('Boxplots of WER and DER by Experiment')\n",
    "    plt.legend(title='Experiment')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_boxplot_experiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a column if it is not needed\n",
    "df_c = df.drop(columns=['Filename']).copy()\n",
    "correlation_matrix = df_c.corr()\n",
    "\n",
    "# Create a heatmap for the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Correlation Matrix of Performance Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of time passed verify transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = pd.read_csv(\"outputs/time_data.csv\")\n",
    "\n",
    "# Function to convert HH:MM:SS to total seconds\n",
    "def time_to_seconds(time_str):\n",
    "    try:\n",
    "        h, m, s = map(int, time_str.split(':'))\n",
    "        return h * 3600 + m * 60 + s\n",
    "    except:\n",
    "        return np.nan  # Return NaN if time_str is not a valid format\n",
    "\n",
    "df_time['Verification_sec'] = df_time['Verification_time'].apply(time_to_seconds)\n",
    "\n",
    "# Drop rows where 'Verification_sec' is NaN\n",
    "df_time = df_time.dropna(subset=['Verification_sec'])\n",
    "\n",
    "df_time.loc[:, 'Ratio'] = df_time['Verification_sec'] / df_time['Duration_sec'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new row for the \"All\" category\n",
    "df_all = df_time.copy()\n",
    "df_all['Experiment'] = 'All'\n",
    "\n",
    "# Concatenate the original data with the \"All\" data\n",
    "df_combined = pd.concat([df_all, df_time])\n",
    "default_palette = sns.color_palette(\"deep\")  \n",
    "palette = {experiment: (default_palette[0] if experiment != 'All' else default_palette[3]) for experiment in df_combined['Experiment'].unique()}\n",
    "\n",
    "# Plot the box plot with \"All\" as an additional category\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_combined, x='Experiment', y='Ratio', palette=palette, hue='Experiment')\n",
    "sns.stripplot(data=df_combined, x='Experiment', y='Ratio', size=4, linewidth=1, dodge=True, jitter=True, edgecolor='k', color='gray')\n",
    "plt.title('Ratio of Verification Time to Recording Duration')\n",
    "plt.xlabel('Experiment')\n",
    "plt.ylabel('Time Spent / Recording Duration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_time['Ratio'].dropna(), kde=True, bins=20)\n",
    "plt.title('Distribution of Time Spent as ratio of Recording Duration')\n",
    "plt.xlabel('Time Spent Verifying / Recording Duration (Ratio)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
