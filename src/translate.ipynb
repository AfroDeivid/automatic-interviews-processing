{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word (.docx) to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from utils.translation_helpers import docx_to_csv\n",
    "\n",
    "word_file = \"../data/Parkinson/en/7-1_script_interview_clinique_1 English.docx\"\n",
    "docx_to_csv(word_file, data_directory=\"../data\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\envs\\seam\\lib\\site-packages\\transformers\\deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import SeamlessM4Tv2ForTextToText, AutoProcessor\n",
    "import torch\n",
    "from utils.translation_helpers import translate_by_row_csv, translate_with_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8739452f772346208d28cca7d7be6b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SeamlessM4Tv2ForTextToText.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "cuda = True\n",
    "\n",
    "if cuda and torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token length: 4096\n",
      "Max new tokens: 256\n"
     ]
    }
   ],
   "source": [
    "# Check the max position embeddings (equivalent to max token length)\n",
    "max_length = model.config.max_position_embeddings\n",
    "print(f\"Max token length: {max_length}\")\n",
    "\n",
    "# Check the max new tokens (equivalent to max OUTPUT token length)\n",
    "model.config.max_new_tokens\n",
    "print(f\"Max new tokens: {model.config.max_new_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chuncks translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.translation_helpers import split_text_into_chunks, translation, parse_and_write_translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"fra\" # French\n",
    "target_lang = \"eng\" # English\n",
    "#path_file = \"../results/Parkinson/fr/7-2_script_interview_clinique_1_13-08-2020.csv\" # long file\n",
    "path_file = \"../results/Parkinson/fr/7-1_script_interview_clinique_3_21-08-2020.csv\" # short file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/Parkinson/fr\\7-1_script_interview_clinique_1.csv  max tokens:  104 at line:  227\n",
      "average tokens:  16.1123595505618\n",
      "../results/Parkinson/fr\\7-1_script_interview_clinique_2_21-08-2020.csv  max tokens:  109 at line:  55\n",
      "average tokens:  13.65625\n",
      "../results/Parkinson/fr\\7-1_script_interview_clinique_3_21-08-2020.csv  max tokens:  72 at line:  32\n",
      "average tokens:  19.658536585365855\n",
      "../results/Parkinson/fr\\7-1_script_interview_clinique_3_21-08-2020_fra_to_eng.csv  max tokens:  68 at line:  27\n",
      "average tokens:  16.470588235294116\n",
      "../results/Parkinson/fr\\7-1_script_interview_clinique_3_21-08-2020_fra_to_eng2022222.csv  max tokens:  70 at line:  20\n",
      "average tokens:  17.964285714285715\n",
      "../results/Parkinson/fr\\7-1_script_interview_clinique_3_21-08-2020_fra_to_eng_80_tokensmax.csv  max tokens:  27 at line:  4\n",
      "average tokens:  11.972972972972974\n",
      "../results/Parkinson/fr\\7-1_script_interview_clinique_4_21-08-2020.csv  max tokens:  63 at line:  3\n",
      "average tokens:  14.181818181818182\n",
      "../results/Parkinson/fr\\7-1_script_interview_clinique_5_21-08-2020.csv  max tokens:  78 at line:  55\n",
      "average tokens:  15.106796116504855\n",
      "../results/Parkinson/fr\\7-2_script_interview_clinique_1_13-08-2020.csv  max tokens:  118 at line:  51\n",
      "average tokens:  20.88362068965517\n",
      "../results/Parkinson/fr\\7-2_script_interview_clinique_2_13-08-2020.csv  max tokens:  61 at line:  2\n",
      "average tokens:  19.6\n",
      "../results/Parkinson/fr\\7-2_script_interview_clinique_3_13-08-2020.csv  max tokens:  321 at line:  153\n",
      "average tokens:  24.261627906976745\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def get_files(directory, extensions):\n",
    "    \"\"\"Get a list of files in the specified directory and its subdirectories with given extensions.\"\"\"\n",
    "    files = []\n",
    "\n",
    "    for root, dirs, files_in_dir in os.walk(directory):\n",
    "        for file in files_in_dir:\n",
    "            if any(file.endswith(ext) for ext in extensions):\n",
    "                files.append(os.path.join(root, file))\n",
    "                \n",
    "    return files\n",
    "\n",
    "# def to see the max numbers of tokens per row in the csv file\n",
    "files = get_files(\"../results/Parkinson/fr\", [\".csv\"])\n",
    "\n",
    "for file in files:\n",
    "    max_tokens = 0\n",
    "    list_tokens = []\n",
    "    with open(file, \"r\") as f:\n",
    "        index = 0\n",
    "        for line in f:\n",
    "            index += 1\n",
    "            tokens = len(line.split())\n",
    "            list_tokens.append(tokens)\n",
    "            if tokens > max_tokens:\n",
    "                max_tokens = tokens\n",
    "                index_max = index\n",
    "    print(file,\" max tokens: \",max_tokens,\"at line: \",index_max)\n",
    "    print(\"average tokens: \",sum(list_tokens)/len(list_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 47\n",
      "\n",
      "Number of tokens: 30\n",
      "Number of tokens: 38\n",
      "Number of tokens: 38\n",
      "Number of tokens: 40\n",
      "Number of tokens: 36\n",
      "Number of tokens: 40\n",
      "Number of tokens: 40\n",
      "Number of tokens: 28\n",
      "Number of tokens: 32\n",
      "Number of tokens: 38\n",
      "Number of tokens: 28\n",
      "Number of tokens: 30\n",
      "Number of tokens: 32\n",
      "Number of tokens: 28\n",
      "Number of tokens: 36\n",
      "Number of tokens: 14\n",
      "Number of tokens: 38\n",
      "Number of tokens: 36\n",
      "Number of tokens: 30\n",
      "Number of tokens: 24\n",
      "Number of tokens: 32\n",
      "Number of tokens: 52\n",
      "Number of tokens: 38\n",
      "Number of tokens: 36\n",
      "Number of tokens: 12\n",
      "Number of tokens: 36\n",
      "Number of tokens: 38\n",
      "Number of tokens: 28\n",
      "Number of tokens: 26\n",
      "Number of tokens: 26\n",
      "Number of tokens: 30\n",
      "Number of tokens: 38\n",
      "Number of tokens: 38\n",
      "Number of tokens: 26\n",
      "Number of tokens: 38\n",
      "Number of tokens: 34\n",
      "Number of tokens: 38\n",
      "Number of tokens: 24\n",
      "Number of tokens: 24\n",
      "Number of tokens: 24\n",
      "Number of tokens: 22\n",
      "Number of tokens: 26\n",
      "Number of tokens: 40\n",
      "Number of tokens: 36\n",
      "Number of tokens: 34\n",
      "Number of tokens: 44\n",
      "Number of tokens: 26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[Examinatrice]: Donc, toujours concernant cette sensation de présence dans le dos, ce vieux monsieur en costume gris.',\n",
       " \"Est-ce que vous avez l'impression, parfois, que c'est cette personne, cette présence essaye d'interagir avec vous, de communiquer avec vous ?\",\n",
       " '[Patiente]: Pas vraiment, parce qu’ils parlent quand même entre eux, mais très doucement. [Examinatrice]: Le monsieur en gris ?',\n",
       " '[Patiente]: Oui mais il parle avec les autres qui sont avec ce monsieur gris. Je le vois, je le vois chaque soir, mais en principe il est tout seul.',\n",
       " \"[Examinatrice]: Il est tout seul. D'accord. Donc, il n’essaye pas de vous parler ? [Patiente]: Non.\",\n",
       " \"[Examinatrice]: Est-ce que vous avez l'impression qu'il essaie de lire dans vos pensées ou qu'il sait déjà à quoi vous pensez ?\",\n",
       " '[Patiente]: Euh, je ne sais pas. Des fois, je me demande : Est-ce que qu’ils savent déjà ce qui se passe demain ?',\n",
       " '[Examinatrice]: Ça veut dire ? [Patiente]: Oui, parce que. Comment l’expliquer ?',\n",
       " '[Examinatrice]: Comme s’il savait ce qui allait se passer demain. Qui pouvait prédire ce qui allait se passer ?',\n",
       " '[Patiente]: Oui, quelque chose comme ça ? Prédire ? Oui, ou bien il fait peut-être un calcul. Un calcul pour savoir une moyenne ?',\n",
       " \"Je ne sais pas quoi dire. Enfin, je ne sais pas. C'est peut-être nul ce que je dis.\",\n",
       " '[Examinatrice]: Non, mais vous dites ce que vous pensez. Alors, comment vous expliquer ce phénomène ?',\n",
       " '[Patiente]: Avec le vieux monsieur ? [Examinatrice]: Oui. [Patiente]: Alors, moi je crois.',\n",
       " \"[Examinatrice]: Pourquoi est-ce que ça vous arrive ? Qu'est-ce que vous en pensez ?\",\n",
       " \"[Patiente]: Oui, ça, c'est une bonne question. Je ne sais pas. Je ne sais pas ce qu'ils veulent me dire.\",\n",
       " 'Jamais eu le courage de leurs poser la question.',\n",
       " \"Parce qu'il y a une copine qui m’demandé : Tu sais ce qu'ils veulent faire chez toi ? Tu demandes, pourquoi vous êtes ici ?\",\n",
       " \"Mais moi, ça m’effraie quand même un peu. [Examinatrice]: Qu'est-ce qui vous effraie, de demander ?\",\n",
       " 'de poser des questions ? [Patiente]: De poser cette question ? Je ne sais pas. La réponse peut être.',\n",
       " \"[Examinatrice]: Est-ce que vous croyez que c'est réel, en fait ?\",\n",
       " \"[Patiente]: Non, je ne crois pas que c'est réel, parce que ce n’est pas très sophistiqué.\",\n",
       " \"Par exemple, les filles avec les tubes qui voulaient me couper les cheveux, ça, c'est des tubes qu'on a utilisé dans des laboratoires en 1965 ou quelque chose comme ça, grand comme ça, en verre.\",\n",
       " \"Je pense que, si c'était réel, ça aurait été beaucoup plus moderne. [Examinatrice]: Mais vous y croyez quand même, quand ça vous arrive.\",\n",
       " \"Parce que, l'envie de poser des questions au monsieur en gris, l'envie de demander : Qu'est-ce que tu veux ?\",\n",
       " 'Qu’es-que ce qui se passe ?',\n",
       " \"[Patiente]: Oui, moi j'aimerais que ça s'arrête, parce que je voudrais de nouveau être tranquille chez moi, etc.\",\n",
       " '[Examinatrice]: Mais, vous pensez qu’en posant des questions, vous aurez des réponses ? [Patiente]: Justement, je ne sais pas.',\n",
       " 'Je n’ai peut-être pas le courage de poser la question. [Examinatrice]: D’accord.',\n",
       " \"[Patiente]: Peut-être si je n'étais pas seul. En général, je suis seul.\",\n",
       " \"Peut-être, si ce n'était pas le cas, alors je le tirerais par les mains.\",\n",
       " \"Et puis, je lui dirais, écoutez, qu'est-ce que vous voulez ? Ou vous vous prononcez ?\",\n",
       " \"Ou bien vous foutez le camp, c'est mon appart, c'est moi qui paye le loyer, voilà, c’est moi le boss.\",\n",
       " \"[Examinatrice]: D'accord, ok. [Patiente]: En plus, je pense que je lui aurais déjà apris à faire le ménage.\",\n",
       " 'Si je pensais que c’était vrai. [Examinatrice]: Maintenant, on va revenir au début.',\n",
       " 'Donc, il y a eu ce monsieur en gris récemment, durant les quatre dernières semaines. Mais, avant cela, il y a eu les deux filles et les deux hommes.',\n",
       " 'Donc, pareil, est-ce qu’en termes de durée, combien de temps est ce que ces présences-là restent-ils ?',\n",
       " 'Donc, il y a quatre personnes en même temps. Vous sentez la présence de quatre personnes en même temps ? [Patiente]: Ils sont venus le même jour.',\n",
       " 'Je dirai le même soir. Puis, Ils sont tous partis aussi au même moment, les deux.',\n",
       " \"[Examinatrice]: Combien de temps ils ont duré à chaque fois qu'ils sont là ?\",\n",
       " '[Patiente]: Je ne voulais pas me faire couper les cheveux par celle-là.',\n",
       " \"Alors, je crois que j'ai lutté pour éviter de me faire couper mes cheveux.\",\n",
       " 'Mais, un soir, j’ai décidé de faire semblant de dormir pour regarder ce qui allait se passer.',\n",
       " 'Soudain, j’ai entendu un bruit de ciseau. Tchac [Examinatrice]: Donc pour vous, ils ont pu vous couper les cheveux ?',\n",
       " \"[Patiente]: Oui, mais je n’ai pas vu les mèches de cheveux qui venaient d’être coupé, c'était comme avant.\",\n",
       " \"[Examinatrice]: Vos cheveux étaient tout à fait normaux ? [Patiente]: Oui, Mais après c'était bon. Ils sont partis.\",\n",
       " \"[Examinatrice]: Et ça, vous y avez pensé immédiatement quand c'est arrivé, qu’il voulait vous couper les cheveux, ou c’est après que vous avez réfléchi ?\",\n",
       " \"[Patiente]: Non, je pense que j'ai réfléchi et que je me suis dit...\"]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 40\n",
    "chunks = split_text_into_chunks(path_file, max_length, processor, source_lang)\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "\n",
    "for text in chunks:\n",
    "    tokens = processor(text, return_tensors=\"pt\", src_lang=source_lang)\n",
    "    print(f\"Number of tokens: {len(tokens['input_ids'][0])}\")\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the chunks\n",
    "translated_chunks = []\n",
    "for chunk in chunks:\n",
    "    translated_text = translation(source_lang, target_lang, chunk, model, processor, True)\n",
    "    translated_chunks.append(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Examiner]: So, still about this feeling of presence in the back, this old gentleman in a gray suit.',\n",
       " 'Do you sometimes feel that this person, this presence, is trying to interact with you, to communicate with you?',\n",
       " \"[Patient]: Not really, because they're still talking to each other, but very softly.\",\n",
       " '[Patient]: Yes, but he talks to the others who are with this gray gentleman. I see him, I see him every night, but in principle he is alone.',\n",
       " \"[Examiner]: He's all alone. Okay. So he's not trying to talk to you? [Patient]: No.\",\n",
       " \"Do you feel like he's trying to read your mind or does he already know what you're thinking?\",\n",
       " \"[Patient]: Uh, I don't know. Sometimes I wonder: Do they already know what's going to happen tomorrow?\",\n",
       " '[Examiner]: What does it mean? [Patient]: Yes, because. How do I explain it?',\n",
       " 'As if he knew what was going to happen tomorrow. Who could predict what was going to happen?',\n",
       " \"[Patient]: Yes, something like that? Predict? Yes, or maybe he's doing a calculation. A calculation to find out an average?\",\n",
       " \"I don't know what to say, I don't know, maybe what I'm saying is wrong.\",\n",
       " '[Examiner]: No, but you say what you think. So how do you explain this phenomenon?',\n",
       " '[Patient]: With the old gentleman? [Examiner]: Yes. [Patient]: Well, I think so.',\n",
       " '[Examinator]: Why is this happening to you? What do you think?',\n",
       " \"[Patient]: Yes, that's a good question. I don't know. I don't know what they want to tell me.\",\n",
       " 'Never had the courage to ask their question.',\n",
       " 'Because there\\'s a girlfriend who asked me, \"Do you know what they want to do to you?\"',\n",
       " \"But I'm a little scared anyway.\",\n",
       " \"I don't know. The answer may be.\",\n",
       " '[Examiner]: Do you believe that it is real, in fact?',\n",
       " \"[Patient]: No, I don't think it's real, because it's not very sophisticated.\",\n",
       " \"For example, the girls with the tubes who wanted to cut my hair, that's, that's tubes that were used in laboratories in 1965 or something like that, as big as that, made of glass.\",\n",
       " 'I think if it was real, it would have been a lot more modern.',\n",
       " 'Because, the urge to ask questions to the gentleman in gray, the urge to ask: What do you want?',\n",
       " 'Whats going on?',\n",
       " '[Patient]: Yes, I would like it to stop, because I would like to be at home again, etc.',\n",
       " \"[Examiner]: But do you think that by asking questions, you will get answers? [Patient]: Exactly, I don't know.\",\n",
       " 'I may not have the courage to ask the question. [Examiner]: Okay.',\n",
       " \"[Patient]: Maybe if I wasn't alone. I'm usually alone.\",\n",
       " \"Maybe, if it wasn't the case, then I'd pull him by the hand.\",\n",
       " 'And then, I would say to him, listen, what do you want? Or do you pronounce yourself?',\n",
       " 'Or you can leave, its my apartment, I pay the rent, here, Im the boss.',\n",
       " \"[Examiner]: Okay, okay. [Patient]: Besides, I think I'd already taught him how to do the housework.\",\n",
       " \"If I thought it was true. [Examinator]: Now, we'll go back to the beginning.\",\n",
       " 'So there was this gentleman in gray recently, during the last four weeks, but before that, there were the two girls and the two men.',\n",
       " 'So, the same, in terms of duration, how long do these presences stay?',\n",
       " 'So there are four people at the same time. Do you feel the presence of four people at the same time?',\n",
       " 'Then, they all left at the same time, both of them.',\n",
       " '[Examiner]: How long did they last each time they were there?',\n",
       " '[Patient]: I didnt want to get my hair cut by that one.',\n",
       " 'So, I think I struggled to avoid getting my hair cut.',\n",
       " 'But one night, I decided to pretend to be asleep to watch what was going to happen.',\n",
       " 'Suddenly, I heard the sound of scissors.',\n",
       " \"[Patient]: Yes, but I didn't see the strands of hair that had just been cut, it was the same as before.\",\n",
       " '[Examiner]: Your hair was perfectly normal? [Patient]: Yes, but afterwards it was fine. They left.',\n",
       " 'And did you think about it immediately when it happened, that he wanted to cut your hair, or did you think about it afterwards?',\n",
       " '[Patient]: No, I think I thought about it and I said to myself...']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 28\n",
      "Number of tokens: 28\n",
      "Number of tokens: 26\n",
      "Number of tokens: 54\n",
      "Number of tokens: 20\n",
      "Number of tokens: 26\n",
      "Number of tokens: 24\n",
      "Number of tokens: 42\n",
      "Number of tokens: 46\n",
      "Number of tokens: 42\n",
      "Number of tokens: 42\n",
      "Number of tokens: 40\n",
      "Number of tokens: 30\n",
      "Number of tokens: 24\n",
      "Number of tokens: 38\n",
      "Number of tokens: 30\n",
      "Number of tokens: 50\n",
      "Number of tokens: 18\n",
      "Number of tokens: 36\n",
      "Number of tokens: 28\n",
      "Number of tokens: 38\n",
      "Number of tokens: 32\n",
      "Number of tokens: 28\n",
      "Number of tokens: 26\n",
      "Number of tokens: 32\n",
      "Number of tokens: 48\n",
      "Number of tokens: 34\n",
      "Number of tokens: 34\n",
      "Number of tokens: 50\n",
      "Number of tokens: 38\n",
      "Number of tokens: 36\n",
      "Number of tokens: 16\n",
      "Number of tokens: 28\n",
      "Number of tokens: 20\n",
      "Number of tokens: 30\n",
      "Number of tokens: 22\n"
     ]
    }
   ],
   "source": [
    "for text in translated_chunks:\n",
    "    tokens = processor(text, return_tensors=\"pt\", src_lang=source_lang)\n",
    "    print(f\"Number of tokens: {len(tokens['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and write the translated text\n",
    "output_csv = f\"{os.path.splitext(path_file)[0]}_{source_lang}_to_{target_lang}_80_tokensmax_sinpuntos.csv\"\n",
    "parse_and_write_translated_text(translated_chunks, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation row by row CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'source_lang = \"fra\" # French\\ntarget_lang = \"eng\" # English\\npath_file = \"../../results/Parkinson/fr/7-1_script_interview_clinique_3_21-08-2020.csv\"\\n\\ntranslate_by_row_csv(path_file, source_lang, target_lang, model, processor, cuda)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"source_lang = \"fra\" # French\n",
    "target_lang = \"eng\" # English\n",
    "path_file = \"../../results/Parkinson/fr/7-1_script_interview_clinique_3_21-08-2020.csv\"\n",
    "\n",
    "translate_by_row_csv(path_file, source_lang, target_lang, model, processor, cuda)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
