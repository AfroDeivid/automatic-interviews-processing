{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word (.docx) to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translation_helpers import docx_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results\\\\Parkinson\\\\en\\\\7-1_script_interview_clinique_1 English.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_file = \"../../data/Parkinson/en/7-1_script_interview_clinique_1 English.docx\"\n",
    "\n",
    "docx_to_csv(word_file, data_directory=\"../../data\")\n",
    "# To save in the main results directory\n",
    "#docx_to_csv(word_file, output_directory=\"../../results\", data_directory=\"../../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\envs\\seam\\lib\\site-packages\\transformers\\deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import SeamlessM4Tv2ForTextToText, AutoProcessor\n",
    "import torch\n",
    "from translation_helpers import translate_by_row_csv, translate_with_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f559df4e8c47c4ae04050c6aa33cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SeamlessM4Tv2ForTextToText.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "cuda = True\n",
    "\n",
    "if cuda and torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token length: 4096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the max position embeddings (equivalent to max token length)\n",
    "max_length = model.config.max_position_embeddings\n",
    "print(f\"Max token length: {max_length}\")\n",
    "\n",
    "model.config.max_new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chuncks translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translation_helpers import split_text_into_chunks, translation, parse_and_write_translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"fra\" # French\n",
    "target_lang = \"eng\" # English\n",
    "#path_file = \"../../results/Parkinson/fr/7-2_script_interview_clinique_1_13-08-2020.csv\" # long file\n",
    "path_file = \"../../results/Parkinson/fr/7-1_script_interview_clinique_3_21-08-2020.csv\" # short file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../results/Parkinson/fr\\7-1_script_interview_clinique_1.csv  max tokens:  104 at line:  226\n",
      "../../results/Parkinson/fr\\7-1_script_interview_clinique_2_21-08-2020.csv  max tokens:  109 at line:  55\n",
      "../../results/Parkinson/fr\\7-1_script_interview_clinique_3_21-08-2020.csv  max tokens:  72 at line:  32\n",
      "../../results/Parkinson/fr\\7-1_script_interview_clinique_3_21-08-2020_fra_to_eng.csv  max tokens:  1 at line:  1\n",
      "../../results/Parkinson/fr\\7-1_script_interview_clinique_4_21-08-2020.csv  max tokens:  63 at line:  3\n",
      "../../results/Parkinson/fr\\7-1_script_interview_clinique_4_21-08-2020_eng.csv  max tokens:  60 at line:  3\n",
      "../../results/Parkinson/fr\\7-1_script_interview_clinique_5_21-08-2020.csv  max tokens:  78 at line:  55\n",
      "../../results/Parkinson/fr\\7-2_script_interview_clinique_1_13-08-2020.csv  max tokens:  118 at line:  51\n",
      "../../results/Parkinson/fr\\7-2_script_interview_clinique_2_13-08-2020.csv  max tokens:  61 at line:  2\n",
      "../../results/Parkinson/fr\\7-2_script_interview_clinique_3_13-08-2020.csv  max tokens:  321 at line:  152\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def get_files(directory, extensions):\n",
    "    \"\"\"Get a list of files in the specified directory and its subdirectories with given extensions.\"\"\"\n",
    "    files = []\n",
    "\n",
    "    for root, dirs, files_in_dir in os.walk(directory):\n",
    "        for file in files_in_dir:\n",
    "            if any(file.endswith(ext) for ext in extensions):\n",
    "                files.append(os.path.join(root, file))\n",
    "                \n",
    "    return files\n",
    "\n",
    "# def to see the max numbers of tokens per row in the csv file\n",
    "files = get_files(\"../../results/Parkinson/fr\", [\".csv\"])\n",
    "\n",
    "for file in files:\n",
    "    max_tokens = 0\n",
    "    with open(file, \"r\") as f:\n",
    "        index = 0\n",
    "        for line in f:\n",
    "            index += 1\n",
    "            tokens = len(line.split())\n",
    "            if tokens > max_tokens:\n",
    "                max_tokens = tokens\n",
    "                index_max = index\n",
    "    print(file,\" max tokens: \",max_tokens,\"at line: \",index_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 10\n",
      "Number of tokens: 190\n",
      "Number of tokens: 170\n",
      "Number of tokens: 176\n",
      "Number of tokens: 184\n",
      "Number of tokens: 172\n",
      "Number of tokens: 118\n",
      "Number of tokens: 172\n",
      "Number of tokens: 178\n",
      "Number of tokens: 196\n",
      "Number of tokens: 76\n"
     ]
    }
   ],
   "source": [
    "max_length = 200 \n",
    "chunks = split_text_into_chunks(path_file, max_length, processor, source_lang)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "for text in chunks:\n",
    "    tokens = processor(text, return_tensors=\"pt\", src_lang=source_lang)\n",
    "    print(f\"Number of tokens: {len(tokens['input_ids'][0])}\")\n",
    "\n",
    "# Translate the chunks\n",
    "translated_chunks = []\n",
    "for chunk in chunks:\n",
    "    translated_text = translation(source_lang, target_lang, chunk, model, processor, True)\n",
    "    translated_chunks.append(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<<SPEAKER:Examinatrice>>\\nDonc, toujours concernant cette sensation de présence dans le dos, ce vieux monsieur en costume gris. Est-ce que vous avez l'impression, parfois, que c'est cette personne, cette présence essaye d'interagir avec vous, de communiquer avec vous ?\\n\\n<<SPEAKER:Patiente>>\\nPas vraiment, parce qu’ils parlent quand même entre eux, mais très doucement.\\n\\n<<SPEAKER:Examinatrice>>\\nLe monsieur en gris ?\\n\\n<<SPEAKER:Patiente>>\\nOui mais il parle avec les autres qui sont avec ce monsieur gris. Je le vois, je le vois chaque soir, mais en principe il est tout seul.\\n\\n<<SPEAKER:Examinatrice>>\\nIl est tout seul. D'accord. Donc, il n’essaye pas de vous parler ?\",\n",
       " \"<<SPEAKER:Patiente>>\\nNon.\\n\\n<<SPEAKER:Examinatrice>>\\nEst-ce que vous avez l'impression qu'il essaie de lire dans vos pensées ou qu'il sait déjà à quoi vous pensez ?\\n\\n<<SPEAKER:Patiente>>\\nEuh, je ne sais pas. Des fois, je me demande : Est-ce que qu’ils savent déjà ce qui se passe demain ?\\n\\n<<SPEAKER:Examinatrice>>\\nÇa veut dire ?\\n\\n<<SPEAKER:Patiente>>\\nOui, parce que. Comment l’expliquer ?\\n\\n<<SPEAKER:Examinatrice>>\\nComme s’il savait ce qui allait se passer demain. Qui pouvait prédire ce qui allait se passer ?\",\n",
       " \"<<SPEAKER:Patiente>>\\nOui, quelque chose comme ça ? Prédire ? Oui, ou bien il fait peut-être un calcul. Un calcul pour savoir une moyenne ? Je ne sais pas quoi dire. Enfin, je ne sais pas. C'est peut-être nul ce que je dis.\\n\\n<<SPEAKER:Examinatrice>>\\nNon, mais vous dites ce que vous pensez. Alors, comment vous expliquer ce phénomène ?\\n\\n<<SPEAKER:Patiente>>\\nAvec le vieux monsieur ?\\n\\n<<SPEAKER:Examinatrice>>\\nOui.\\n\\n<<SPEAKER:Patiente>>\\nAlors, moi je crois.\\n\\n<<SPEAKER:Examinatrice>>\\nPourquoi est-ce que ça vous arrive ? Qu'est-ce que vous en pensez ?\",\n",
       " \"<<SPEAKER:Patiente>>\\nOui, ça, c'est une bonne question. Je ne sais pas. Je ne sais pas ce qu'ils veulent me dire. Jamais eu le courage de leurs poser la question. Parce qu'il y a une copine qui m’demandé : Tu sais ce qu'ils veulent faire chez toi ? Tu demandes, pourquoi vous êtes ici ? Mais moi, ça m’effraie quand même un peu.\\n\\n<<SPEAKER:Examinatrice>>\\nQu'est-ce qui vous effraie, de demander ? de poser des questions ?\\n\\n<<SPEAKER:Patiente>>\\nDe poser cette question ? Je ne sais pas. La réponse peut être.\\n\\n<<SPEAKER:Examinatrice>>\\nEst-ce que vous croyez que c'est réel, en fait ?\",\n",
       " \"<<SPEAKER:Patiente>>\\nNon, je ne crois pas que c'est réel, parce que ce n’est pas très sophistiqué. Par exemple, les filles avec les tubes qui voulaient me couper les cheveux, ça, c'est des tubes qu'on a utilisé dans des laboratoires en 1965 ou quelque chose comme ça, grand comme ça, en verre. Je pense que, si c'était réel, ça aurait été beaucoup plus moderne.\\n\\n<<SPEAKER:Examinatrice>>\\nMais vous y croyez quand même, quand ça vous arrive. Parce que, l'envie de poser des questions au monsieur en gris, l'envie de demander : Qu'est-ce que tu veux ? Qu’es-que ce qui se passe ?\",\n",
       " \"<<SPEAKER:Patiente>>\\nOui, moi j'aimerais que ça s'arrête, parce que je voudrais de nouveau être tranquille chez moi, etc.\\n\\n<<SPEAKER:Examinatrice>>\\nMais, vous pensez qu’en posant des questions, vous aurez des réponses ?\\n\\n<<SPEAKER:Patiente>>\\nJustement, je ne sais pas. Je n’ai peut-être pas le courage de poser la question.\\n\\n<<SPEAKER:Examinatrice>>\\nD’accord.\",\n",
       " \"<<SPEAKER:Patiente>>\\nPeut-être si je n'étais pas seul. En général, je suis seul. Peut-être, si ce n'était pas le cas, alors je le tirerais par les mains. Et puis, je lui dirais, écoutez, qu'est-ce que vous voulez ? Ou vous vous prononcez ? Ou bien vous foutez le camp, c'est mon appart, c'est moi qui paye le loyer, voilà, c’est moi le boss.\\n\\n<<SPEAKER:Examinatrice>>\\nD'accord, ok.\\n\\n<<SPEAKER:Patiente>>\\nEn plus, je pense que je lui aurais déjà apris à faire le ménage. Si je pensais que c’était vrai.\",\n",
       " \"<<SPEAKER:Examinatrice>>\\nMaintenant, on va revenir au début. Donc, il y a eu ce monsieur en gris récemment, durant les quatre dernières semaines. Mais, avant cela, il y a eu les deux filles et les deux hommes. Donc, pareil, est-ce qu’en termes de durée, combien de temps est ce que ces présences-là restent-ils ? Donc, il y a quatre personnes en même temps. Vous sentez la présence de quatre personnes en même temps ?\\n\\n<<SPEAKER:Patiente>>\\nIls sont venus le même jour. Je dirai le même soir. Puis, Ils sont tous partis aussi au même moment, les deux.\\n\\n<<SPEAKER:Examinatrice>>\\nCombien de temps ils ont duré à chaque fois qu'ils sont là ?\",\n",
       " \"<<SPEAKER:Patiente>>\\nJe ne voulais pas me faire couper les cheveux par celle-là. Alors, je crois que j'ai lutté pour éviter de me faire couper mes cheveux. Mais, un soir, j’ai décidé de faire semblant de dormir pour regarder ce qui allait se passer. Soudain, j’ai entendu un bruit de ciseau. Tchac\\n\\n<<SPEAKER:Examinatrice>>\\nDonc pour vous, ils ont pu vous couper les cheveux ?\\n\\n<<SPEAKER:Patiente>>\\nOui, mais je n’ai pas vu les mèches de cheveux qui venaient d’être coupé, c'était comme avant.\\n\\n<<SPEAKER:Examinatrice>>\\nVos cheveux étaient tout à fait normaux ?\\n\\n<<SPEAKER:Patiente>>\\nOui, Mais après c'était bon. Ils sont partis.\",\n",
       " \"<<SPEAKER:Examinatrice>>\\nEt ça, vous y avez pensé immédiatement quand c'est arrivé, qu’il voulait vous couper les cheveux, ou c’est après que vous avez réfléchi ?\\n\\n<<SPEAKER:Patiente>>\\nNon, je pense que j'ai réfléchi et que je me suis dit...\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"So, always about this feeling of presence in the back, this old man in a gray suit. Do you sometimes get the impression that it's this person, this presence trying to interact with you, to communicate with you? So, not really, because they're still talking to each other, but very softly. So, the gray man, yes, but he's talking to the others who are with this gray man. I see him, I see him every night, but basically he's all alone.\",\n",
       " \"No. Do you have the impression that he's trying to read your mind or that he already knows what you're thinking? Well, I don't know. Sometimes I wonder: Do they already know what's going to happen tomorrow?\",\n",
       " '\"SPEAKER: Patient\". \"Yes, something like that? Predicting?\" \"Yes, or maybe he\\'s doing a calculation. A calculation to find out an average?\" \"I don\\'t know what to say\". \"Well, I don\\'t know. Maybe it\\'s nothing\". \"SPEAKER: Examiner\". \"No, but you say what you think\". \"So how do you explain this phenomenon?\" \"With the old man?\" \"SPEAKER: Examiner\". \"Yes\". \"SPEAKER: Examiner\". \"So I believe you\". \"SPEAKER: Why do you think that?\" \"Examiner: What happens?\"',\n",
       " \"I don't know. I don't know what they want to tell me. I've never had the courage to ask them the question. Because there's a girlfriend who asked me: Do you know what they want to do at home? You ask, why are you here? But me, it scares me a little bit.\",\n",
       " \"No, I don't think it's real, because it's not very sophisticated. For example, the girls with the tubes who wanted to cut my hair, these are tubes that were used in laboratories in 1965 or something like that, this big, made of glass. I think if it was real, it would have been much more modern.\",\n",
       " \"Yes, I'd like it to stop, because I'd like to be quiet at home again, etc. But do you think that by asking questions, you'll get answers?\",\n",
       " 'SPEAKER: Patient. Maybe if I wasn\\'t alone. In general, I\\'m alone. Maybe if I wasn\\'t, then I\\'d pull him by the hands. And then I\\'d say, \"Listen, what do you want?\" Or you\\'d say, \"Or you\\'re out of here, it\\'s my apartment, I\\'m paying the rent, here I am, I\\'m the boss\".',\n",
       " \"So, we're going to go back to the beginning. So, there was this gentleman in gray recently, during the last four weeks. But, before that, there were the two girls and the two men. So, the same, is it in terms of duration, how long do these presences stay? So now, there are four people at the same time. Do you feel the presence of four people at the same time?\",\n",
       " \"I didn't want to get my hair cut by that one. So, I think I struggled to avoid getting my hair cut. But, one night, I decided to pretend to sleep to see what was going to happen. Suddenly, I heard a scissor noise.\",\n",
       " 'SPEAKER: Examiner. And did you think about it immediately when it happened, that he wanted to cut your hair, or did you think about it afterwards? SPEAKER: Patient. No, I think I thought about it and I said to myself...']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parse and write the translated text\n",
    "output_csv = f\"{os.path.splitext(path_file)[0]}_{source_lang}_to_{target_lang}.csv\"\n",
    "parse_and_write_translated_text(translated_chunks, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation row by row CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 19/19 [01:56<00:00,  6.12s/row]\n"
     ]
    }
   ],
   "source": [
    "source_lang = \"fra\" # French\n",
    "target_lang = \"eng\" # English\n",
    "path_file = \"../../results/Parkinson/fr/7-1_script_interview_clinique_3_21-08-2020.csv\"\n",
    "\n",
    "translate_by_row_csv(path_file, source_lang, target_lang, model, processor, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
