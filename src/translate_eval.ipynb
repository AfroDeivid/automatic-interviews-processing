{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from sacrebleu import sentence_bleu\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Computes BLEU score for a single reference-hypothesis pair.\n",
    "    \"\"\"\n",
    "    return sentence_bleu(hypothesis, [reference]).score / 100  # Normalize to [0, 1]\n",
    "\n",
    "def compute_meteor(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Computes METEOR score for a single reference-hypothesis pair.\n",
    "    Tokenizes the input to match the expected format.\n",
    "    \"\"\"\n",
    "    # Tokenize both reference and hypothesis\n",
    "    tokenized_reference = word_tokenize(reference)\n",
    "    tokenized_hypothesis = word_tokenize(hypothesis)\n",
    "\n",
    "    # Calculate METEOR score\n",
    "    return meteor_score([tokenized_reference], tokenized_hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\david\\anaconda3\\envs\\translation_eval\\lib\\site-packages\\bleurt\\score.py:160: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Reading checkpoint bleurt_checkpoints/BLEURT-20.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Could not find BLEURT checkpoint bleurt_checkpoints/BLEURT-20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the BLEURT scorer with a pre-trained checkpoint\u001b[39;00m\n\u001b[0;32m      4\u001b[0m bleurt_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbleurt_checkpoints/BLEURT-20\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m scorer \u001b[38;5;241m=\u001b[39m \u001b[43mBleurtScorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbleurt_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_bleurt\u001b[39m(reference, hypothesis):\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    Computes BLEURT score for a single reference-hypothesis pair.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\translation_eval\\lib\\site-packages\\bleurt\\score.py:161\u001b[0m, in \u001b[0;36mBleurtScorer.__init__\u001b[1;34m(self, checkpoint, predict_fn)\u001b[0m\n\u001b[0;32m    158\u001b[0m   checkpoint \u001b[38;5;241m=\u001b[39m _get_default_checkpoint()\n\u001b[0;32m    160\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading checkpoint \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(checkpoint))\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_bleurt_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_seq_length\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    163\u001b[0m vocab_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_file\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\translation_eval\\lib\\site-packages\\bleurt\\checkpoint.py:84\u001b[0m, in \u001b[0;36mread_bleurt_config\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_bleurt_config\u001b[39m(path):\n\u001b[0;32m     83\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Reads and checks config file from a BLEURT checkpoint.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(path), \\\n\u001b[0;32m     85\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find BLEURT checkpoint \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path)\n\u001b[0;32m     86\u001b[0m   config_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, CONFIG_FILE)\n\u001b[0;32m     87\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(config_path), \\\n\u001b[0;32m     88\u001b[0m       (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find BLEURT config file \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Are you sure \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is a valid checkpoint?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(config_path, path)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Could not find BLEURT checkpoint bleurt_checkpoints/BLEURT-20"
     ]
    }
   ],
   "source": [
    "from bleurt.score import BleurtScorer\n",
    "\n",
    "# Load the BLEURT scorer with a pre-trained checkpoint\n",
    "bleurt_checkpoint = \"bleurt_checkpoints/BLEURT-20\"\n",
    "scorer = BleurtScorer(bleurt_checkpoint)\n",
    "\n",
    "def compute_bleurt(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Computes BLEURT score for a single reference-hypothesis pair.\n",
    "    \"\"\"\n",
    "    score = scorer.score(references=[reference], candidates=[hypothesis])\n",
    "    return score[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_from_folders(reference_folder, hypothesis_folder, lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Evaluate translations by processing reference and hypothesis files directly from folders.\n",
    "\n",
    "    Parameters:\n",
    "    - reference_folder (str): Path to the folder containing reference files.\n",
    "    - hypothesis_folder (str): Path to the folder containing hypothesis files.\n",
    "    - lang (str): Language for BERTScore (default is \"en\").\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with per-file scores for METEOR and BLEU.\n",
    "    \"\"\"\n",
    "    # Find all files in both folders\n",
    "    reference_files = {\n",
    "        os.path.basename(f).replace(\" English\", \"\"): os.path.join(reference_folder, f)\n",
    "        for f in os.listdir(reference_folder)\n",
    "    }\n",
    "    hypothesis_files = {\n",
    "        os.path.basename(f): os.path.join(hypothesis_folder, f)\n",
    "        for f in os.listdir(hypothesis_folder)\n",
    "    }\n",
    "\n",
    "    # Ensure matching filenames exist in both folders\n",
    "    common_files = set(reference_files.keys()) & set(hypothesis_files.keys())\n",
    "    if not common_files:\n",
    "        raise ValueError(\"No matching files found in the reference and hypothesis folders.\")\n",
    "\n",
    "    # Initialize per-file scores\n",
    "    file_scores = {}\n",
    "\n",
    "    # Process each matching file\n",
    "    for filename in tqdm(common_files, desc=\"Evaluating Files\"):\n",
    "        print(f\"Processing: {filename}\")\n",
    "\n",
    "        with open(reference_files[filename], 'r', encoding='utf-8') as ref_file:\n",
    "            reader = csv.DictReader(ref_file)\n",
    "            reference = [row[\"Text\"] for row in reader]\n",
    "        with open(hypothesis_files[filename], 'r', encoding='utf-8') as hyp_file:\n",
    "            reader = csv.DictReader(hyp_file)\n",
    "            hypothesis = [row[\"Text\"] for row in reader]\n",
    "\n",
    "        # Concatenate all texts into single strings\n",
    "        reference = \" \".join(reference)\n",
    "        hypothesis = \" \".join(hypothesis)\n",
    "\n",
    "        #print(f\"Reference: {reference}\")\n",
    "        #print(f\"Hypothesis: {hypothesis}\")\n",
    "        # Compute metrics\n",
    "        meteor = compute_meteor(reference, hypothesis)\n",
    "        bleu = compute_bleu(reference, hypothesis)\n",
    "\n",
    "        # Save per-file scores\n",
    "        file_scores[filename] = {\n",
    "            \"METEOR\": meteor,\n",
    "            \"BLEU\": bleu,\n",
    "        }\n",
    "\n",
    "    return file_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Files:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 7-1_script_interview_clinique_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m hypothesis_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../results/Parkinson/translation_fra_to_eng\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Evaluate translations\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_from_folders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m scores\n",
      "Cell \u001b[1;32mIn[4], line 40\u001b[0m, in \u001b[0;36mevaluate_from_folders\u001b[1;34m(reference_folder, hypothesis_folder, lang)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(hypothesis_files[filename], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hyp_file:\n\u001b[0;32m     39\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(hyp_file)\n\u001b[1;32m---> 40\u001b[0m     hypothesis \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Concatenate all texts into single strings\u001b[39;00m\n\u001b[0;32m     43\u001b[0m reference \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(reference)\n",
      "Cell \u001b[1;32mIn[4], line 40\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(hypothesis_files[filename], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hyp_file:\n\u001b[0;32m     39\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(hyp_file)\n\u001b[1;32m---> 40\u001b[0m     hypothesis \u001b[38;5;241m=\u001b[39m [\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mText\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Concatenate all texts into single strings\u001b[39;00m\n\u001b[0;32m     43\u001b[0m reference \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(reference)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Text'"
     ]
    }
   ],
   "source": [
    "# Define folder paths\n",
    "reference_folder = \"../results/Parkinson/en\"\n",
    "hypothesis_folder = \"../results/Parkinson/translation_fra_to_eng\"\n",
    "\n",
    "# Evaluate translations\n",
    "scores = evaluate_from_folders(reference_folder, hypothesis_folder, lang=\"en\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translation_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
