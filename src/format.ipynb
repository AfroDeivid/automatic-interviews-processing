{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from metrics_helpers import process_folder, load_data_time, compute_der\n",
    "from format_helpers import get_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions vs References: Performance evaluation (ASR & Diarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing evaluation folder (Predictions vs References)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The csv file that you want to compare\n",
    "directories = ['../results/Compassion', '../results/OBE1', '../results/OBE2']\n",
    "# Specify the name if trying to compare different models of ASR & Diarization\n",
    "pred_folder = '../performance_evaluation/predictions'\n",
    "\n",
    "pred_files = []\n",
    "for directory in directories:\n",
    "    pred_files.append(get_files(directory, 'csv'))\n",
    "pred_files = [item for sublist in pred_files for item in sublist]\n",
    "\n",
    "os.makedirs(pred_folder, exist_ok=True)\n",
    "for file in pred_files:\n",
    "    shutil.copy(file, pred_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR and Diarization metrics + Visual Tool to be better compare & understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined HTML file saved as ../performance_evaluation/predictions\\visual comparison\\Diarization_S301final.html\n",
      "Processed file: S301final.csv\n",
      "Combined HTML file saved as ../performance_evaluation/predictions\\visual comparison\\Diarization_S302con.html\n",
      "Processed file: S302con.csv\n"
     ]
    }
   ],
   "source": [
    "# Hands-on parameters to better estimate the performance of the ASR system\n",
    "# As the diariasation task may skip whole sentences worse unfairly the WER metric\n",
    "max_insert_length = 3  # Exclude insertions longer than 3 words from the WER calculation\n",
    "\n",
    "prediction_folder = \"../performance_evaluation/predictions\"\n",
    "reference_folder = \"../performance_evaluation/references\"\n",
    "\n",
    "metric = process_folder(prediction_folder, reference_folder, max_insert_length)\n",
    "metric.to_csv('../performance_evaluation/WER_metric.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Dialogue DER Analysis in CSV instead than in HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialgue_DER(reference_file, prediction_file, output_file):\n",
    "    df_ref, df_pred = load_data_time(reference_file, prediction_file)\n",
    "    dialogue_df, error_durations = compute_der(df_ref, df_pred)\n",
    "\n",
    "    # Display error durations and DER\n",
    "    print(\"\\nError Durations and DER:\")\n",
    "    for key, value in error_durations.items():\n",
    "        if key == 'DER':\n",
    "            print(f\"{key}: {value:.2%}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value:.3f} seconds\")\n",
    "\n",
    "    # Optionally, save the DataFrame to a CSV file\n",
    "    dialogue_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error Durations and DER:\n",
      "DER: 7.18%\n",
      "Reference Speech Duration: 342.280 seconds\n",
      "Missed Duration: 1.060 seconds\n",
      "False Alarm Duration: 0.260 seconds\n",
      "Confusion Duration: 23.240 seconds\n"
     ]
    }
   ],
   "source": [
    "reference_file = '../S301final_C.csv'\n",
    "prediction_file = '../S301final.csv'\n",
    "output_file = 'dia_S301final.csv'\n",
    "\n",
    "dialgue_DER(reference_file, prediction_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (To think about) Conventional way to calculate WER\n",
    "Don't take in acount the fact that the diarization model may skip complete sentences due to bad quality of audio and overlapp & thereffore worsen unfairly the WER from the ASR.\n",
    "\n",
    "**TODO:** *Further test with different models to try to solve this problem*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "def calculate_wer_for_csv(hypothesis_file, reference_file):\n",
    "    \"\"\"\n",
    "    Calculate the WER for the entire content of the 'Content' column in a CSV file.\n",
    "    \"\"\"\n",
    "    with open(hypothesis_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        hyp = ' '.join(row['Content'] for row in reader)\n",
    "\n",
    "    with open(reference_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        ref = ' '.join(row['Content'] for row in reader)\n",
    "\n",
    "    return wer_metric.compute(references=[ref], predictions=[hyp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03026634382566586"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_file= \"../S301final_C.csv\"\n",
    "hypothesis_file  = \"../results/Compassion/S301final.csv\"\n",
    "\n",
    "calculate_wer_for_csv(hypothesis_file, reference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23446893787575152"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_file = \"../results/Compassion/S302con.csv\"\n",
    "reference_file = \"../S302con_C.csv\"\n",
    "\n",
    "calculate_wer_for_csv(hypothesis_file, reference_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Useful) Problems with excel format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_semicolon_to_comma(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Converts a CSV file with semicolons as delimiters to a comma-separated CSV.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(infile, delimiter=';')\n",
    "        writer = csv.writer(outfile, delimiter=',')\n",
    "        \n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def find_problematic_line(input_file):\n",
    "    \"\"\"\n",
    "    Reads a file line by line to find the line that causes a UnicodeDecodeError.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            # If the line contains the problematic character, print the line number and content\n",
    "            if '\\ufffd' in line:  # '\\ufffd' is the replacement character for decoding errors\n",
    "                print(f\"Problematic line at line {line_number}: {line}\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"No problematic lines found.\")\n",
    "\n",
    "def remove_bom_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Removes the Byte Order Mark (BOM) from a file if it exists.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Check for BOM (UTF-8 BOM is \\xef\\xbb\\xbf)\n",
    "    if content.startswith(b'\\xef\\xbb\\xbf'):\n",
    "        print(f\"BOM found in {file_path}, removing it...\")\n",
    "        content = content[3:]  # Remove the first three bytes (BOM)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(content)\n",
    "        print(f\"BOM successfully removed from {file_path}.\")\n",
    "    else:\n",
    "        print(f\"No BOM found in {file_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
