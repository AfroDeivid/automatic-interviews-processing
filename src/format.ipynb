{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import difflib\n",
    "import pandas as pd\n",
    "\n",
    "# Run this once if not already installed\n",
    "#nltk.download('punkt') \n",
    "#nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_csv_content(file_path):\n",
    "    \"\"\"\n",
    "    Flatten the 'Content' column of a CSV file into a single string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        # Join all the content from the 'Content' column into one plain text string\n",
    "        return ' '.join(row['Content'].strip() for row in reader)\n",
    "\n",
    "def compare_and_highlight_changes(reference, prediction):\n",
    "    \"\"\"\n",
    "    Compare reference and prediction texts and highlight changes required to match the prediction to the reference.\n",
    "    Track insertions (I), deletions (D), and replacements (R).\n",
    "    \"\"\"\n",
    "    ref_words = word_tokenize(reference)\n",
    "    pred_words = word_tokenize(prediction)\n",
    "    \n",
    "    # Use difflib's SequenceMatcher to identify changes between the two tokenized texts\n",
    "    matcher = difflib.SequenceMatcher(None, ref_words, pred_words)\n",
    "    diff = matcher.get_opcodes()\n",
    "    \n",
    "    highlighted_diff = []\n",
    "    total_D, total_I, total_R = 0, 0, 0  # Initialize counters for deletions, insertions, and replacements\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in diff:\n",
    "        if tag == 'equal':\n",
    "            highlighted_diff.extend(ref_words[i1:i2])\n",
    "        elif tag == 'replace':\n",
    "            # Show what needs to be replaced in the prediction to match the reference\n",
    "            highlighted_diff.append(f\"<span style='background-color: yellow;'>[REPLACE: {' '.join(pred_words[j1:j2])} -> {' '.join(ref_words[i1:i2])}]</span>\")\n",
    "            total_R += max(i2 - i1, j2 - j1)\n",
    "        elif tag == 'delete':\n",
    "            # Show what needs to be inserted into the prediction to match the reference\n",
    "            highlighted_diff.append(f\"<span style='background-color: green;'>[INSERT: {' '.join(ref_words[i1:i2])}]</span>\")\n",
    "            total_D += i2 - i1\n",
    "        elif tag == 'insert':\n",
    "            # Show what needs to be deleted from the prediction\n",
    "            highlighted_diff.append(f\"<span style='background-color: red;'>[DELETE: {' '.join(pred_words[j1:j2])}]</span>\")\n",
    "            total_I += j2 - j1\n",
    "\n",
    "    return ' '.join(highlighted_diff), total_D, total_I, total_R\n",
    "\n",
    "def calculate_wer_and_generate_html(prediction_file, reference_file, output_file):\n",
    "    \"\"\"\n",
    "    Flatten prediction and reference CSVs, calculate WER, and generate an HTML output highlighting the changes needed \n",
    "    to match the prediction to the reference.\n",
    "    \"\"\"\n",
    "    # Step 1: Flatten both prediction and reference CSVs into plain text\n",
    "    reference_text = flatten_csv_content(reference_file)\n",
    "    prediction_text = flatten_csv_content(prediction_file)\n",
    "    \n",
    "    # Step 2: Compare the plain texts and highlight what changes are needed\n",
    "    highlighted_diff, total_D, total_I, total_R = compare_and_highlight_changes(reference_text, prediction_text)\n",
    "    \n",
    "    # Step 3: Calculate WER\n",
    "    total_words = len(word_tokenize(reference_text))\n",
    "    wer = (total_D + total_I + total_R) / total_words if total_words > 0 else 0\n",
    "    \n",
    "    # Step 4: Save results to an HTML file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"<html><body><div>{highlighted_diff}</div></body></html>\")\n",
    "\n",
    "    # Step 5: Print summary\n",
    "    print(f\"Visual differences saved to {output_file}\")\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    print(f\"Total Words (Reference): {total_words}\")\n",
    "    print(f\"Total Insertions (D, from Reference to Prediction): {total_D}\")\n",
    "    print(f\"Total Deletions (I, removed from Prediction): {total_I}\")\n",
    "    print(f\"Total Replacements (R, in Prediction): {total_R}\")\n",
    "    print(f\"WER: {wer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual differences saved to prediction_vs_reference.html\n",
      "\n",
      "=== Summary ===\n",
      "Total Words (Reference): 579\n",
      "Total Insertions (D, from Reference to Prediction): 102\n",
      "Total Deletions (I, removed from Prediction): 1\n",
      "Total Replacements (R, in Prediction): 36\n",
      "WER: 0.2401\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "reference_file= \"../S302con_C.csv\"\n",
    "prediction_file  = \"../results/Compassion/S302con.csv\"\n",
    "output_file = \"prediction_vs_reference.html\"\n",
    "\n",
    "# Run WER calculation and generate HTML output\n",
    "calculate_wer_and_generate_html(prediction_file, reference_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing S301final.csv...\n",
      "Visual differences saved to ./comparison_results\\S301final_diff.html\n",
      "\n",
      "=== Summary ===\n",
      "Total Words (Reference): 1071\n",
      "Total Insertions (D, from Reference to Prediction): 18\n",
      "Total Deletions (I, removed from Prediction): 3\n",
      "Total Replacements (R, in Prediction): 31\n",
      "WER: 0.0486\n",
      "Processing S302con.csv...\n",
      "Visual differences saved to ./comparison_results\\S302con_diff.html\n",
      "\n",
      "=== Summary ===\n",
      "Total Words (Reference): 732\n",
      "Total Insertions (D, from Reference to Prediction): 103\n",
      "Total Deletions (I, removed from Prediction): 4\n",
      "Total Replacements (R, in Prediction): 93\n",
      "WER: 0.2732\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import difflib\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def flatten_csv_content(file_path):\n",
    "    \"\"\"\n",
    "    Flatten the 'Content' column of a CSV file into a single string, including speaker information.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        content = []\n",
    "        for row in reader:\n",
    "            speaker = row.get('Speaker', '').strip()\n",
    "            text = row.get('Content', '').strip()\n",
    "            if speaker:\n",
    "                content.append(f\"Speaker {speaker}: {text}\")\n",
    "            else:\n",
    "                content.append(text)\n",
    "        return ' '.join(content)\n",
    "    \n",
    "def compare_and_highlight_changes(reference, prediction):\n",
    "    \"\"\"\n",
    "    Compare reference and prediction texts and highlight changes required to match the prediction to the reference.\n",
    "    Track insertions (I), deletions (D), and replacements (R).\n",
    "    \"\"\"\n",
    "    ref_words = word_tokenize(reference)\n",
    "    pred_words = word_tokenize(prediction)\n",
    "    \n",
    "    # Use difflib's SequenceMatcher to identify changes between the two tokenized texts\n",
    "    matcher = difflib.SequenceMatcher(None, ref_words, pred_words)\n",
    "    diff = matcher.get_opcodes()\n",
    "    \n",
    "    highlighted_diff = []\n",
    "    total_D, total_I, total_R = 0, 0, 0  # Initialize counters for deletions, insertions, and replacements\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in diff:\n",
    "        if tag == 'equal':\n",
    "            highlighted_diff.extend(ref_words[i1:i2])\n",
    "        elif tag == 'replace':\n",
    "            highlighted_diff.append(f\"<span style='background-color: yellow;'>[REPLACE: {' '.join(pred_words[j1:j2])} -> {' '.join(ref_words[i1:i2])}]</span>\")\n",
    "            total_R += max(i2 - i1, j2 - j1)\n",
    "        elif tag == 'delete':\n",
    "            highlighted_diff.append(f\"<span style='background-color: green;'>[INSERT: {' '.join(ref_words[i1:i2])}]</span>\")\n",
    "            total_D += i2 - i1\n",
    "        elif tag == 'insert':\n",
    "            highlighted_diff.append(f\"<span style='background-color: red;'>[DELETE: {' '.join(pred_words[j1:j2])}]</span>\")\n",
    "            total_I += j2 - j1\n",
    "    \n",
    "    return ' '.join(highlighted_diff), total_D, total_I, total_R\n",
    "\n",
    "def calculate_wer_and_generate_html(prediction_file, reference_file, output_file):\n",
    "    \"\"\"\n",
    "    Flatten prediction and reference CSVs, calculate WER, and generate an HTML output highlighting the changes needed \n",
    "    to match the prediction to the reference.\n",
    "    \"\"\"\n",
    "    # Step 1: Flatten both prediction and reference CSVs into plain text\n",
    "    reference_text = flatten_csv_content(reference_file)\n",
    "    prediction_text = flatten_csv_content(prediction_file)\n",
    "    \n",
    "    # Step 2: Compare the plain texts and highlight what changes are needed\n",
    "    highlighted_diff, total_D, total_I, total_R = compare_and_highlight_changes(reference_text, prediction_text)\n",
    "    \n",
    "    # Step 3: Calculate WER\n",
    "    total_words = len(word_tokenize(reference_text))\n",
    "    wer = (total_D + total_I + total_R) / total_words if total_words > 0 else 0\n",
    "    \n",
    "    # Step 4: Save results to an HTML file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"<html><body><div>{highlighted_diff}</div></body></html>\")\n",
    "    \n",
    "    # Step 5: Print summary\n",
    "    print(f\"Visual differences saved to {output_file}\")\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    print(f\"Total Words (Reference): {total_words}\")\n",
    "    print(f\"Total Insertions (D, from Reference to Prediction): {total_D}\")\n",
    "    print(f\"Total Deletions (I, removed from Prediction): {total_I}\")\n",
    "    print(f\"Total Replacements (R, in Prediction): {total_R}\")\n",
    "    print(f\"WER: {wer:.4f}\")\n",
    "\n",
    "def process_multiple_files(prediction_dir, reference_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Process multiple CSV files in the specified directories.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get list of prediction files\n",
    "    prediction_files = [f for f in os.listdir(prediction_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    for pred_file in prediction_files:\n",
    "        pred_path = os.path.join(prediction_dir, pred_file)\n",
    "        ref_path = os.path.join(reference_dir, pred_file)\n",
    "        \n",
    "        if os.path.exists(ref_path):\n",
    "            output_file = os.path.join(output_dir, pred_file.replace('.csv', '_diff.html'))\n",
    "            print(f\"Processing {pred_file}...\")\n",
    "            calculate_wer_and_generate_html(pred_path, ref_path, output_file)\n",
    "        else:\n",
    "            print(f\"Reference file {pred_file} not found in {reference_dir}. Skipping.\")\n",
    "\n",
    "# Example usage:\n",
    "prediction_dir = \"../Zprediction\"\n",
    "reference_dir = \"../Ztrue\"\n",
    "output_dir = \"./comparison_results\"\n",
    "\n",
    "# Run WER calculation and generate HTML outputs for all files\n",
    "process_multiple_files(prediction_dir, reference_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison HTML file saved as comparison_output.html\n"
     ]
    }
   ],
   "source": [
    "def time_to_seconds(t):\n",
    "    \"\"\"\n",
    "    Convert time format 'HH:MM:SS' to seconds.\n",
    "    \"\"\"\n",
    "    h, m, s = t.split(':')\n",
    "    return int(h) * 3600 + int(m) * 60 + float(s)\n",
    "\n",
    "def align_rows_by_time(df_ref, df_pred):\n",
    "    \"\"\"\n",
    "    Align rows from reference and prediction DataFrames based on overlapping time intervals.\n",
    "    \"\"\"\n",
    "    aligned_pairs = []\n",
    "    ref_intervals = list(zip(df_ref['Start'], df_ref['End'], df_ref.index))\n",
    "    pred_intervals = list(zip(df_pred['Start'], df_pred['End'], df_pred.index))\n",
    "\n",
    "    ref_idx = 0\n",
    "    pred_idx = 0\n",
    "\n",
    "    while ref_idx < len(ref_intervals) and pred_idx < len(pred_intervals):\n",
    "        ref_start, ref_end, ref_i = ref_intervals[ref_idx]\n",
    "        pred_start, pred_end, pred_i = pred_intervals[pred_idx]\n",
    "\n",
    "        # Skip if start or end times are None\n",
    "        if ref_start is None or ref_end is None:\n",
    "            ref_idx += 1\n",
    "            continue\n",
    "        if pred_start is None or pred_end is None:\n",
    "            pred_idx += 1\n",
    "            continue\n",
    "\n",
    "        # Ensure intervals have a minimum duration\n",
    "        if ref_end <= ref_start:\n",
    "            ref_end = ref_start + 0.001  # Add small epsilon\n",
    "        if pred_end <= pred_start:\n",
    "            pred_end = pred_start + 0.001  # Add small epsilon\n",
    "\n",
    "        # Check for overlap with a small tolerance\n",
    "        overlap_start = max(ref_start, pred_start)\n",
    "        overlap_end = min(ref_end, pred_end)\n",
    "\n",
    "        if overlap_start < overlap_end + 0.001:\n",
    "            # Intervals overlap\n",
    "            aligned_pairs.append((df_ref.loc[ref_i], df_pred.loc[pred_i]))\n",
    "            ref_idx += 1\n",
    "            pred_idx += 1\n",
    "        elif ref_end <= pred_start:\n",
    "            # Reference interval ends before prediction interval starts\n",
    "            aligned_pairs.append((df_ref.loc[ref_i], None))  # Missing in prediction\n",
    "            ref_idx += 1\n",
    "        else:\n",
    "            # Prediction interval ends before reference interval starts\n",
    "            aligned_pairs.append((None, df_pred.loc[pred_i]))  # Extra in prediction\n",
    "            pred_idx += 1\n",
    "\n",
    "    # Handle any remaining intervals\n",
    "    while ref_idx < len(ref_intervals):\n",
    "        ref_start, ref_end, ref_i = ref_intervals[ref_idx]\n",
    "        aligned_pairs.append((df_ref.loc[ref_i], None))\n",
    "        ref_idx += 1\n",
    "\n",
    "    while pred_idx < len(pred_intervals):\n",
    "        pred_start, pred_end, pred_i = pred_intervals[pred_idx]\n",
    "        aligned_pairs.append((None, df_pred.loc[pred_i]))\n",
    "        pred_idx += 1\n",
    "\n",
    "    return aligned_pairs\n",
    "\n",
    "def format_diff(ref_text, hyp_text):\n",
    "    \"\"\"\n",
    "    Format differences between reference and hypothesis texts for HTML display.\n",
    "    \"\"\"\n",
    "    ref_words = ref_text.split()\n",
    "    hyp_words = hyp_text.split()\n",
    "    sm = difflib.SequenceMatcher(None, hyp_words, ref_words) # Highlight the changes needed in the prediction to match the reference\n",
    "    opcodes = sm.get_opcodes()\n",
    "    diff_html = ''\n",
    "    for tag, i1, i2, j1, j2 in opcodes:\n",
    "        if tag == 'equal':\n",
    "            diff_html += ' ' + ' '.join(hyp_words[i1:i2])\n",
    "        elif tag == 'replace':\n",
    "            diff_html += ' <span style=\"background-color: yellow;\">[<s>{}</s> → {}]</span>'.format(\n",
    "                ' '.join(hyp_words[i1:i2]), ' '.join(ref_words[j1:j2]))\n",
    "        elif tag == 'insert':\n",
    "            diff_html += ' <span style=\"background-color: lightgreen;\">[{}]</span>'.format(\n",
    "                ' '.join(ref_words[j1:j2]))\n",
    "        elif tag == 'delete':\n",
    "            diff_html += ' <span style=\"background-color: lightcoral;\">[<s>{}</s>]</span>'.format(\n",
    "                ' '.join(hyp_words[i1:i2]))\n",
    "    return diff_html.strip()\n",
    "\n",
    "def format_cell_diff(ref_value, pred_value):\n",
    "    \"\"\"\n",
    "    Format cell differences for columns other than 'Content'.\n",
    "    \"\"\"\n",
    "    if str(ref_value) != str(pred_value):\n",
    "        if str(pred_value) != '':\n",
    "        # Modified (e.g., speaker misattribution)\n",
    "            return f\"<td style='background-color: mediumpurple;'>{pred_value} → {ref_value}</td>\"\n",
    "        else:  \n",
    "            # Missing in prediction (missed speech)\n",
    "            return f\"<td style='background-color: lightcoral;'> → {ref_value}</td>\"\n",
    "    else:\n",
    "        # No change\n",
    "        return f\"<td>{ref_value}</td>\"  # Leave empty\n",
    "\n",
    "def compare_csv_files(reference_file, prediction_file, output_file):\n",
    "    \"\"\"\n",
    "    Compare two CSV files and generate an HTML output highlighting the differences.\n",
    "    \"\"\"\n",
    "    # Read CSV files\n",
    "    df_ref = pd.read_csv(reference_file, encoding='utf-8')\n",
    "    df_pred = pd.read_csv(prediction_file, encoding='utf-8')\n",
    "\n",
    "    # Convert times to seconds\n",
    "    df_ref['Start'] = df_ref['Start Time'].apply(time_to_seconds)\n",
    "    df_ref['End'] = df_ref['End Time'].apply(time_to_seconds)\n",
    "    # Adjust zero-length intervals\n",
    "    df_ref.loc[df_ref['End'] <= df_ref['Start'], 'End'] = df_ref['Start'] + 0.001\n",
    "\n",
    "    df_pred['Start'] = df_pred['Start Time'].apply(time_to_seconds)\n",
    "    df_pred['End'] = df_pred['End Time'].apply(time_to_seconds)\n",
    "    # Adjust zero-length intervals\n",
    "    df_pred.loc[df_pred['End'] <= df_pred['Start'], 'End'] = df_pred['Start'] + 0.001\n",
    "\n",
    "    # Align rows\n",
    "    aligned_pairs = align_rows_by_time(df_ref, df_pred)\n",
    "\n",
    "    # Prepare HTML output\n",
    "    html_output = \"<html><head><style>\"\n",
    "    html_output += \"table {border-collapse: collapse; width: 100%;}\"\n",
    "    html_output += \"th, td {border: 1px solid black; padding: 5px; text-align: left;}\"\n",
    "    html_output += \"</style></head><body>\"\n",
    "    html_output += \"<table>\"\n",
    "    html_output += \"<tr><th>Index</th><th>Start Time</th><th>End Time</th><th>Speaker</th><th>Content</th></tr>\"\n",
    "\n",
    "    for idx, (ref_row, pred_row) in enumerate(aligned_pairs):\n",
    "        html_output += \"<tr>\"\n",
    "        # Index\n",
    "        html_output += f\"<td>{idx + 1}</td>\"\n",
    "\n",
    "        # Start Time\n",
    "        ref_start_time = ref_row['Start Time'] if ref_row is not None else ''\n",
    "        pred_start_time = pred_row['Start Time'] if pred_row is not None else ''\n",
    "        html_output += format_cell_diff(ref_start_time, pred_start_time)\n",
    "\n",
    "        # End Time\n",
    "        ref_end_time = ref_row['End Time'] if ref_row is not None else ''\n",
    "        pred_end_time = pred_row['End Time'] if pred_row is not None else ''\n",
    "        html_output += format_cell_diff(ref_end_time, pred_end_time)\n",
    "\n",
    "        # Speaker\n",
    "        ref_speaker = ref_row['Speaker'] if ref_row is not None else ''\n",
    "        pred_speaker = pred_row['Speaker'] if pred_row is not None else ''\n",
    "        html_output += format_cell_diff(ref_speaker, pred_speaker)\n",
    "\n",
    "        # Content\n",
    "        ref_content = ref_row['Content'] if ref_row is not None else ''\n",
    "        pred_content = pred_row['Content'] if pred_row is not None else ''\n",
    "        content_diff_html = format_diff(ref_content, pred_content)\n",
    "        html_output += f\"<td>{content_diff_html}</td>\"\n",
    "\n",
    "        html_output += \"</tr>\"\n",
    "\n",
    "    html_output += \"</table>\"\n",
    "\n",
    "    html_output += \"</body></html>\"\n",
    "\n",
    "    # Save HTML output\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_output)\n",
    "\n",
    "    print(f\"Comparison HTML file saved as {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "reference_file = '../S302con_C.csv'\n",
    "prediction_file = '../S302con.csv'\n",
    "output_file = 'comparison_output.html'\n",
    "\n",
    "compare_csv_files(reference_file, prediction_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\envs\\raw\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wer_for_csv_a(hypothesis_file, reference_file):\n",
    "    \"\"\"\n",
    "    Calculate the WER for the entire content of the 'Content' column in a CSV file.\n",
    "    \"\"\"\n",
    "    with open(hypothesis_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        hyp = ' '.join(row['Content'] for row in reader)\n",
    "\n",
    "    with open(reference_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        ref = ' '.join(row['Content'] for row in reader)\n",
    "\n",
    "    return wer_metric.compute(references=[ref], predictions=[hyp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03026634382566586"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_file= \"../S301final_C.csv\"\n",
    "hypothesis_file  = \"../results/Compassion/S301final.csv\"\n",
    "\n",
    "calculate_wer_for_csv_a(hypothesis_file, reference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23446893787575152"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_file = \"../results/Compassion/S302con.csv\"\n",
    "reference_file = \"../S302con_C.csv\"\n",
    "\n",
    "calculate_wer_for_csv_a(hypothesis_file, reference_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems with excel format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_semicolon_to_comma(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Converts a CSV file with semicolons as delimiters to a comma-separated CSV.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(infile, delimiter=';')\n",
    "        writer = csv.writer(outfile, delimiter=',')\n",
    "        \n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Example usage\n",
    "input_file = \"../S302con_C.csv\"\n",
    "output_file = 'test.csv'\n",
    "\n",
    "convert_csv_semicolon_to_comma(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problematic line at line 22: 20;Compassion;S302con;302;00:02:01;00:02:09;3;but then in the second part the time you had the eyes closed was longer in the first part or�? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_problematic_line(input_file):\n",
    "    \"\"\"\n",
    "    Reads a file line by line to find the line that causes a UnicodeDecodeError.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            # If the line contains the problematic character, print the line number and content\n",
    "            if '\\ufffd' in line:  # '\\ufffd' is the replacement character for decoding errors\n",
    "                print(f\"Problematic line at line {line_number}: {line}\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"No problematic lines found.\")\n",
    "\n",
    "# Example usage\n",
    "input_file = \"../S302con_C.csv\"\n",
    "find_problematic_line(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No BOM found in ../S302con_C.csv.\n"
     ]
    }
   ],
   "source": [
    "def remove_bom_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Removes the Byte Order Mark (BOM) from a file if it exists.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Check for BOM (UTF-8 BOM is \\xef\\xbb\\xbf)\n",
    "    if content.startswith(b'\\xef\\xbb\\xbf'):\n",
    "        print(f\"BOM found in {file_path}, removing it...\")\n",
    "        content = content[3:]  # Remove the first three bytes (BOM)\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(content)\n",
    "        print(f\"BOM successfully removed from {file_path}.\")\n",
    "    else:\n",
    "        print(f\"No BOM found in {file_path}.\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"../S302con_C.csv\"\n",
    "remove_bom_from_file(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
